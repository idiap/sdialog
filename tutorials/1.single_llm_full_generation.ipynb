{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Dialogue Generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first check if our environment is all set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the environment depending on weather we are running in Google Colab or Jupyter Notebook\n",
    "from IPython import get_ipython\n",
    "\n",
    "if \"google.colab\" in str(get_ipython()):\n",
    "    print(\"Running on CoLab\")\n",
    "    # Downloading only the \"output\" directory from the repository\n",
    "    !git init .\n",
    "    !git remote add -f origin https://github.com/Play-Your-Part/tutorials.git\n",
    "    !git config core.sparseCheckout true\n",
    "    !echo \"output\" >> .git/info/sparse-checkout\n",
    "    !git pull origin main\n",
    "\n",
    "    # Installing Ollama\n",
    "    !curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "    # Installing sdialog\n",
    "    !git clone https://github.com/idiap/sdialog.git\n",
    "    %cd sdialog\n",
    "    %pip install -e .\n",
    "    %cd ..\n",
    "else:\n",
    "    print(\"Running in Jupyter Notebook\")\n",
    "    # Little hack to avoid the \"OSError: Background processes not supported.\" error in Jupyter notebooks\"\n",
    "    import os\n",
    "    get_ipython().system = os.system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> âš ï¸ If you're using **Colab**, please, **restart the runtime** once everything above is installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ollama Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the ollama server first, as a background process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "!OLLAMA_KEEP_ALIVE=-1 ollama serve > /dev/null 2>&1 &\n",
    "!sleep 10  # Let's wait a bit for the server to start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the default model (gemma3:27b), in case you want to replace the default one, you can use `sdialog.config` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sdialog\n",
    "\n",
    "# UNCOMMENT THE MODEL YOU WANT TO USE (TO OVERRIDE THE DEFAULT ONE)\n",
    "\n",
    "# use any model from https://ollama.com/library)\n",
    "# sdialog.config.set_llm(\"qwen2.5:14b\")\n",
    "# sdialog.config.set_llm(\"qwen2.5:1.5b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Output (Dialogue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin by defining the JSON objects that we will use to represent the generated dialogues. For now this object will have only three fields: `\"model\"`, `\"seed\"`, `\"scenario\"`, and `\"dialog\"` to store the name of the model and the seed used to generate the dialogue, as well as the scenario associated to the dialogue and the dialogue itself, respectively. More preciselly, the `\"dialog\"` field will contain the list of turns of the conversation in order, with the speaker name and the corresponding utterances. As shown in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_dialogue = {\n",
    "    \"model\": \"qwen2.5:14b\",  # the model used to generate the dialogue\n",
    "    \"seed\": 123,  # the seed used to generated\n",
    "    \"scenario\": \"short hello and good bye conversation\",  # the scenario used to generated the dialogue\n",
    "    \"turns\": [\n",
    "        {\"speaker\": \"Alice\", \"text\": \"Hey Bob!\"},\n",
    "        {\"speaker\": \"Bob\", \"text\": \"Hey Alice!\"},\n",
    "        {\"speaker\": \"Alice\", \"text\": \"Bye Bob!\"},\n",
    "        {\"speaker\": \"Bob\", \"text\": \"Bye bye!\"},\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `pydantic` to properly define our `Dialogue` type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List, Union, Optional\n",
    "\n",
    "class Turn(BaseModel):\n",
    "    speaker: str\n",
    "    text: str\n",
    "\n",
    "class Dialog(BaseModel):\n",
    "    model: str  # the model used to generate the dialogue\n",
    "    seed: int  # the seed used to generated\n",
    "    scenario: Optional[Union[dict, str]] = None  # the scenario used to generated the dialogue\n",
    "    turns: List[Turn]  # the list of turns of the conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a Python `pydantic` class to formally represent our dialogues is quite useful, we can convert any JSON dialogue to our `Dialog` class as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dialogue = Dialog.model_validate(example_dialogue)\n",
    "my_dialogue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or the opposite, convert our `Dialog`s to a `dict` or a JSON as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dialogue.model_dump()  # a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dialogue_json = my_dialogue.model_dump_json(indent=2)  # a string containing the dialog as a JSON object\n",
    "print(my_dialogue_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, of course, create a new `Dialog` from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dialog(\n",
    "    model=\"qwen2.5:14b\",\n",
    "    seed=123,\n",
    "    turns=[\n",
    "        Turn(speaker=\"Alice\", text=\"Hi :)\"),\n",
    "        Turn(speaker=\"Bob\", text=\"Bye! :(\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternativelly, we can use the built-in `Dialog` class from `sdialog`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdialog import Dialog\n",
    "\n",
    "my_dialog = Dialog.model_validate(example_dialogue)\n",
    "my_dialog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which besides providing the exact same functionalities, among other things, allow us to:\n",
    "\n",
    "- Pretty print the dialogue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dialog.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Print it in a vanilla textual form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_dialog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Save it to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# either as a JSON object\n",
    "my_dialog.to_file(\"output/my_dialogue.json\")\n",
    "\n",
    "# or a txt file\n",
    "my_dialog.to_file(\"output/my_dialogue.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_(check created files [`output/my_dialogue.json`](output/my_dialogue.json) and [`output/my_dialogue.txt`](output/my_dialogue.txt))_\n",
    "\n",
    "- Load a dialogue from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dialog = Dialog.from_file(\"output/my_dialogue.json\")\n",
    "my_dialog.print(scenario=True)  # `scenario=True` to also print the metadata stored in scenario field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dialog = Dialog.from_file(\"output/my_dialogue.txt\")\n",
    "my_dialog.print(scenario=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Or simple things like quickly know how long a dialogue is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(my_dialog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to begin working on synthetic `Dialog` (;)) generation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dialogue Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description-driven Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `sdialog`'s built-in `DialogueGenerator` class that we can instantiate using descriptions to generate our dialogues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdialog.generators import DialogGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which takes the following arguments as input:\n",
    "- `model` model name to use (any model tag from [ollama hub](https://ollama.com/library)).\n",
    "- `dialogue_details` the details about the desired dialogue.\n",
    "- `output_format` the output format as a `pydantic` class or JSON scheme, as we did above (`LLMDialogOutput` by default).\n",
    "- `scenario` an optional metadata field that describes the scenario used to generated dialogue.\n",
    "\n",
    "For instance, let's create an instance of `DialogGenerator` to generate conversations between Bob and Alice about her birthday:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialog_generator = DialogGenerator(\n",
    "    dialogue_details=\"The conversation is between a dad (Bob) and his doughter (Alice). \"\n",
    "                     \"Her birthday is coming up and she wants to throw a Star Wars themed party.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can use the build-in `.generate()` method to generate conversations for such instance:\n",
    "\n",
    "_(each time you run the code below, a different dialogue will be generated)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialog = dialog_generator.generate()\n",
    "dialog.print(scenario=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now change the description, now the party has to be about Lord of the Rings, not Star Wars. To do this we can simply pass the new description when calling the generate method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialog = dialog_generator.generate(\n",
    "    \"The conversation is between a dad (Bob) and his doughter (Alice). \"\n",
    "    \"Her birthday is coming up and she wants to throw a Lord of the Rings themed party.\"\n",
    ")\n",
    "dialog.print(scenario=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the `seed` number above to re-generate the exact same dialogue each time as an argument of `generate()`, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialog_generator.generate(\n",
    "        \"The conversation is between a dad (Bob) and his doughter (Alice). \"\n",
    "        \"Her birthday is coming up and she wants to throw a Lord of the Rings themed party.\",\n",
    "        seed=2167019297\n",
    ").print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Role-Playing-based Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal here will be to have to LLM to generate the dialogues by role-playing the different charecters.\n",
    "\n",
    "Each character will be fully defined by its persona, so, the same way started this tutorial by defining what a \"synthetic `Dialog`\" will actually be, we should now define our `Persona`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, the `sdialog` contains a `BasePersona` that we can import to create our own custom persona classes, let's import it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdialog.personas import BasePersona"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's define our concrete `Persona` class now by specifying some useful attributes like a name, role, background, etc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Persona(BasePersona):\n",
    "    name: str = \"\"\n",
    "    role: str = \"\"\n",
    "    background: str = \"\"\n",
    "    personality: str = \"\"\n",
    "    circumstances: str = \"\"\n",
    "    rules: str = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create/instantiate any `Persona` we want for our characters. Let's create our Bob and Alice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "bob_persona = Persona(\n",
    "        name=\"Bob\",\n",
    "        role=\"great dad\",\n",
    "        circumstances=\"Your daughter will talk to you\",\n",
    "        background=\"Computer Science PhD.\",\n",
    "        personality=\"an extremely happy person that likes to help people\",\n",
    ")\n",
    "\n",
    "alice_persona = Persona(\n",
    "    name=\"Alice\",\n",
    "    role=\"lovely daughter\",\n",
    "    circumstances=\"Your birthday is getting closer and you are talking with your dad to organize the party.\"\n",
    "                  \"You want your party to be themed as Lord of The Rings.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we print `bob` we will see that it is automatically converted to natural language description, which is usefull when we want to create the actual prompt for our LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bob_persona)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case we are working with a really complex persona and this default description is not good for your needs, you can overwrite it by defining your own `description()` method as in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonaCustom(BasePersona):\n",
    "    name: str = \"\"\n",
    "    role: str = \"\"\n",
    "\n",
    "    def description(self):\n",
    "        return f\"Your awesome name is {self.name} and your awesome role is being a {self.role}\"\n",
    "\n",
    "awesome_bob = PersonaCustom(\n",
    "        name=\"Bob\",\n",
    "        role=\"great dad\"\n",
    ")\n",
    "\n",
    "# Let's print \"awesome_bob\" persona\n",
    "print(awesome_bob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we now know how to create personas, let's move to the fun part which is actually creating the actual generator.\n",
    "\n",
    "Fortunatelly, we can simply use `sdialog`'s built-in `PersonaDialogGenerator` class to generate our persona-based dialogues as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdialog.generators import PersonaDialogGenerator\n",
    "\n",
    "dialog_generator = PersonaDialogGenerator(\n",
    "    persona_a=bob_persona,\n",
    "    persona_b=alice_persona,\n",
    ")\n",
    "\n",
    "dialog_generator.generate().print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case: Dialogue Generation for STAR Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin this section, make sure you have the STAR dataset downloaded in your system, inside the `datasets` folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's clone the STAR dataset repository\n",
    "!git clone https://github.com/RasaHQ/STAR.git datasets/STAR\n",
    "\n",
    "# Let's check that `dialogues` and `tasks` folders are inside `datasets/STAR`\n",
    "!ls datasets/STAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [STAR](https://arxiv.org/pdf/2010.11853) dataset contains 6652 human-generated dialogues as JSON objects where files are named as `NUMBER.json`.\n",
    "\n",
    "Humans had to follow a well-defined set of instruction to generate the dialogue role playing the system (wizard) and the client (user).\n",
    "\n",
    "For instance, clicking [here](datasets/STAR/dialogues/1.json) we can open the file [`1.json`](datasets/STAR/dialogues/1.json) containing the first dialogue. For now, let's focus only on the `\"Scenario\"` field.\n",
    "\n",
    "For instance, for the dialogue in `1.json` it is as follows:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"Domains\": [  # List of domains\n",
    "        \"doctor\"\n",
    "    ],\n",
    "    \"Happy\": true,  # Wheather or not the dialogue follos a happy path\n",
    "    \"MultiTask\": false,  # Wheather or not this dialogue involves more than one task\n",
    "    \"UserTask\": \"You (Alexis) had an appointment with Dr. Morgan the other day. Unfortunately, you forgot to write down the instructions the doctor gave you. Please followup and find out how often to take your medicine.\",\n",
    "    \"WizardTask\": \"Inform the user of his/her doctor's orders.\",\n",
    "    \"WizardCapabilities\": [  # List of flowcharts describing the each task the Wizard is cable of doing\n",
    "        {\n",
    "        \"Domain\": \"doctor\",\n",
    "        \"SchemaImage\": \"doctor_followup.jpg\",\n",
    "        \"Task\": \"doctor_followup\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "We can use the `STAR` module from `sdialog` to read scenarios object from any dialogue in STAR given it's id given a STAR conversation id as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdialog.datasets import STAR\n",
    "\n",
    "# Let's first indicate where the dataset is located\n",
    "STAR.set_path(\"datasets/STAR/\")\n",
    "\n",
    "# Let's set the first dialogue as the target example\n",
    "TARGET_DIALOG = 1\n",
    "\n",
    "# Let's load the scenario of the first dialog\n",
    "scenario = STAR.get_dialog_scenario(TARGET_DIALOG)\n",
    "scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which corresponds to the following dialogue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dialogue = STAR.get_dialog(1)\n",
    "original_dialogue.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description-driven Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `scenario`, we can see that in this conversation, the user's behavior is defined by instructions given in natural language (`\"UserTask\"`), however, the system/wizard behavior is more rigidly defined as a graph describing the dialogue policy to followed (since system was expected to be more deterministic). These graphs are described as JSON objects storing the graph edges as key:value pairs (source:destination). We can find these graphs in the [`STAR/tasks`](datasets/STAR/tasks) folder.\n",
    "\n",
    "Ideally, we would like our `DialogGenerator` to generate dialogues for each different scenario. That is, given a `scenario` we would like generate multiple dialogues for it.\n",
    "\n",
    "To achieve this, we only need to find a way to describe each `scenario` using natural language so that we can pass it to our `DialogGenerator`.\n",
    "\n",
    "Fortunately, we can use the built-in `get_scenario_description()` method to do this, which takes an `scenario` as input and returns its natural language description containing all the details (including the system behavior described by the graphs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(STAR.get_scenario_description(scenario))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the original graph in JSON describing the system's behavior have been converted to a [DOT](https://en.wikipedia.org/wiki/DOT_(graph_description_language)) description which should be easier to interpret by the LLM, since DOT is a well-known format to describe graphs in plain text. \n",
    "\n",
    "Let's now put these two methods together and create a function that given a STAR dialogue ID will generate a natural language description of the scenario associated to it, simply as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dialog_scenario_description(dialogue_id):\n",
    "    # Get the scenario of the target dialogue\n",
    "    scenario = STAR.get_dialog_scenario(dialogue_id)\n",
    "    # Then return it along its description in natural language\n",
    "    return scenario, STAR.get_scenario_description(scenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this function now we have everything we need to generate synthethic dialogues for STAR that follows the same scenario as a given target real dialogue.\n",
    "\n",
    "For instance, let's say we want to generate dialogues following the same scenario as the first STAR dialogue, we can simply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's get the scenario and description of the first dialogue\n",
    "scenario, description = get_dialog_scenario_description(dialogue_id=1)\n",
    "\n",
    "# le'ts now create a dialogue generator for it\n",
    "dialog_generator = DialogGenerator(\n",
    "    dialogue_details=description,\n",
    "    scenario=scenario\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now generate multiple conversation that follows the same scenario as dialogue 1 of STAR dataset.\n",
    "> **Note**\n",
    "> Run the cell multiple times to get different conversations for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialog = dialog_generator.generate()\n",
    "dialog.print(scenario=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the LLM is able to follow the scenario surprisegnly well, specially for the system part which is guided by a graph with pre-defined responses.\n",
    "\n",
    "Now update the generator to match a more challenging scenario, let's say that of dialogue 5100 that is multi-task and does not follow a happy path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario, description = get_dialog_scenario_description(5100)\n",
    "\n",
    "dialog_generator = DialogGenerator(\n",
    "    dialogue_details=description,\n",
    "    scenario=scenario\n",
    ")\n",
    "scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And generate dialogues for it:\n",
    "\n",
    "_(run multi-times the call to generate different ones)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialog_generator.generate().print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Role-playing-based Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before, in previous section we only had to find a way to describe each `scenario` using natural language so that we can pass it to our `DialogGenerator`.\n",
    "\n",
    "Likewise, now we have to find a way to create the right system and user personas for each scenario which means we have to return the right system and user `Persona`s for a given `scenario`.\n",
    "\n",
    "Fortunately, we can use the built-in `STAR.get_user_persona_for_scenario(scenario)` and `STAR.get_system_persona_for_scenario(scenario)` methods to achieve this.\n",
    "\n",
    "For instance, let's get the user persona for the `scenario` of the first dialogue above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = STAR.get_dialog_scenario(TARGET_DIALOG)\n",
    "\n",
    "user_persona = STAR.get_user_persona_for_scenario(scenario)\n",
    "print(user_persona)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, similar to what we did in the previous subsection, we just need to define a function that given a dialogue ID can return its scenario as well as the system and user persona for it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dialog_scenario_and_personas(dialogue_id):\n",
    "    # Get the scenario of the target dialogue\n",
    "    scenario = STAR.get_dialog_scenario(dialogue_id)\n",
    "    # Get the personas\n",
    "    system = STAR.get_system_persona_for_scenario(scenario)\n",
    "    user = STAR.get_user_persona_for_scenario(scenario)\n",
    "    return scenario, system, user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it, not we can simply create a `PersonaDialogGenerator` using the system and user personas as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get the personas and the scenario\n",
    "scenario, system, user = get_dialog_scenario_and_personas(dialogue_id=1)\n",
    "\n",
    "# le'ts now create a dialogue generator for it\n",
    "dialog_generator = PersonaDialogGenerator(\n",
    "    persona_a=system,\n",
    "    persona_b=user,\n",
    "    scenario=scenario\n",
    ")\n",
    "\n",
    "# let's generate the dialogue\n",
    "dialog_generator.generate().print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it for this tutorial, congrats! ðŸ˜Ž"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving our dialogues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's generate one synthetic dialog for each happy `\"doctor_followup\"` dialog in STAR and save it to disk for later use.\n",
    "\n",
    "Let's first get all happy dialogues for this task using `sdialog`'s built-in `STAR.get_dialogs()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dialogs = STAR.get_dialogs(task_name=\"doctor_followup\", happy=True, multitask=False)\n",
    "print('Total number of happy \"doctor_followup\" dialogues in STAR:', len(original_dialogs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate the dialogues and save them in the path pointed by the `PATH_OUTPUT` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "PATH_OUTPUT = \"output/STAR/full-generation\"\n",
    "\n",
    "path_txt = os.path.join(PATH_OUTPUT, \"txt\")\n",
    "path_json = os.path.join(PATH_OUTPUT, \"json\")\n",
    "os.makedirs(path_txt, exist_ok=True)\n",
    "os.makedirs(path_json, exist_ok=True)\n",
    "\n",
    "for dialog in tqdm(original_dialogs, desc=\"Dialog generation\"):\n",
    "    if os.path.exists(os.path.join(path_txt, f\"{dialog.dialogId}.txt\")):\n",
    "        continue\n",
    "\n",
    "    scenario, description = STAR.get_dialog_scenario_description(dialog.dialogId)\n",
    "    dialog_generator = DialogGenerator(\n",
    "        dialogue_details=description,\n",
    "        scenario=scenario\n",
    "    )\n",
    "    dialog = dialog_generator.generate(id=dialog.dialogId, seed=dialog.dialogId)\n",
    "\n",
    "    # Normalize speaker names in each turn (since their also generated by the LLM)\n",
    "    for turn in dialog.turns:\n",
    "        turn.speaker = \"System\" if \"AI\" in turn.speaker else \"User\"\n",
    "\n",
    "    dialog.to_file(os.path.join(path_json, f\"{dialog.dialogId}.json\"))\n",
    "    dialog.to_file(os.path.join(path_txt, f\"{dialog.dialogId}.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's check the files were generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls output/STAR/full-generation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls output/STAR/full-generation/txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Doctor-Patient Conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose now you have to generate synthetic doctor-patient conversations, is it better to do it using the \"description\" or the \"role-playing\" approach?\n",
    "\n",
    "- How would you define the Patient and Doctor personas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: do your magic!\n",
    "class DoctorPersona(BasePersona):\n",
    "    pass\n",
    "\n",
    "class PatientPersona(BasePersona):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How would you initialize them? (e.g. concrete values for each defined attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doctor = DoctorPersona(\n",
    "    # TODO: assign values to the attribues!\n",
    ")\n",
    "patient = PatientPersona(\n",
    "    # TODO: assign values to the attribues!\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have defined our Personas and created our two doctor and patient personas, wwe can simply create a generator for them as we did before in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# le'ts now create a dialogue generator for our doctor and patients\n",
    "dialog_generator = PersonaDialogGenerator(\n",
    "    persona_a=doctor,\n",
    "    persona_b=patient\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And generate as many dialogues for them: _(running the cell multiple times)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's generate the doctor-patient dialogue\n",
    "dialog_generator.generate().print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Can you think of an `scenario` object for doctor-patient conversations? What would this scenario contain?\n",
    "\n",
    "> ðŸ’¡ **Hint:** what is it that you want to keep track/control when generating the conversations? (different dissises? different outcomes? different skills? etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = {}  # TODO: define your own, perhaps better to use pydantic's BaseModel instead of a dict {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we had such `scenario` then we could define a function that could return the right doctor and patient for the provided scenario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doctor_patient_for_scenario(scenario):\n",
    "    # TODO: do some magic to return the right doctor\n",
    "    #       and patient personas for the given scenario\n",
    "    # doctor = DoctorPersona()\n",
    "    # patient = DoctorPersona()\n",
    "    return doctor, patient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which we could finally use to create generators for different `scenarios`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generator_for_scenario(scenario):\n",
    "    # Get the right doctor and patient personas for the given scenario\n",
    "    doctor, patient = get_doctor_patient_for_scenario(scenario)\n",
    "\n",
    "    # Create and return a dialogue generator for them\n",
    "    return PersonaDialogGenerator(\n",
    "        persona_a=doctor,\n",
    "        persona_b=patient,\n",
    "        scenario=scenario,\n",
    "        # dialogue_details=\"\"  # TODO: optional, in case scenario also requires defining certain properties outside the personas\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = get_generator_for_scenario(scenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which will allow us to generate multiple dialogues belonging to the same `scenario`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, huh? Congrats for finalizing the tutorial! you did a great job! ðŸ˜Ž"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
