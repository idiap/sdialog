{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Dialogue Generation with Multiple Agents\n",
    "\n",
    "<p align=\"right\" style=\"margin-right: 8px;\">\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/idiap/sdialog/blob/main/tutorials/00_overview/2.multi-agent_generation.ipynb\">\n",
    "        <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "    </a>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin, let's first set up our environment..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in Jupyter Notebook\n"
     ]
    }
   ],
   "source": [
    "# Setup the environment depending on weather we are running in Google Colab or Jupyter Notebook\n",
    "from IPython import get_ipython\n",
    "\n",
    "if \"google.colab\" in str(get_ipython()):\n",
    "    print(\"Running on CoLab\")\n",
    "    # Downloading only the \"output\" directory from the repository\n",
    "    !git init .\n",
    "    !git remote add -f origin https://github.com/Play-Your-Part/tutorials.git\n",
    "    !git config core.sparseCheckout true\n",
    "    !echo \"output\" >> .git/info/sparse-checkout\n",
    "    !git pull origin main\n",
    "\n",
    "    # Installing Ollama\n",
    "    !curl -fsSL https://ollama.com/install.sh | sh\n",
    "    # Installing sdialog\n",
    "    !git clone https://github.com/idiap/sdialog.git\n",
    "    %cd sdialog\n",
    "    %pip install -e .\n",
    "    %cd ..\n",
    "\n",
    "else:\n",
    "    print(\"Running in Jupyter Notebook\")\n",
    "    # Little hack to avoid the \"OSError: Background processes not supported.\" error in Jupyter notebooks\"\n",
    "    import os\n",
    "    get_ipython().system = os.system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ⚠️ If you're using **Colab**, please, **restart the runtime** once everything above is installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's first make sure we have the Ollama server running..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start the ollama server\n",
    "!OLLAMA_KEEP_ALIVE=-1 ollama serve > /dev/null 2>&1 &\n",
    "!sleep 10  # Wait a bit for the server to start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change the default sdialog model to `gemma3:12b`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sdialog\n",
    "\n",
    "sdialog.config.llm(\"ollama:gemma3:12b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Role-Play Multi-Agent-based Dialogue Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal here will be, instead of having one LLM to generate the complete dialogue, is to have two LLMs \"talking to each other\" by role-playing different charecters.\n",
    "\n",
    "Each character will be fully defined by its persona, as we did in the previous tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persona"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could create our own `Persona` class as we did in the previous tutorial, or, better let's use the `sdialog`'s built-in one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdialog.personas import Persona"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, let's only create one persona, Bob:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bob_persona = Persona(\n",
    "        name=\"Bob\",\n",
    "        role=\"great dad\",\n",
    "        circumstances=\"Your daughter will talk to you\",\n",
    "        background=\"Computer Science PhD.\",\n",
    "        personality=\"an extremely happy person that likes to help people\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move to the fun part which is actually creating the LLM agent that will play this role.\n",
    "\n",
    "Fortunatelly, we can simply use `sdialog`'s built-in `Agent` class to create an agent for our personas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent\n",
    "\n",
    "In its simplest form, the `Agent` class only takes a `Persona` object and the LLM model name to use for it, and will create the LLM-based agent for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-21 11:35:20] INFO:sdialog.util:Loading Ollama model: gemma3:12b\n"
     ]
    }
   ],
   "source": [
    "from sdialog.agents import Agent\n",
    "\n",
    "bob = Agent(bob_persona)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's talk with Bob a little bit as if we were his daughter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hey sweetie! So great to hear from you!'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bob(\"Hi dad!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Oh, absolutely! Tell me all about it! Let's make it the best birthday ever!\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bob(\"I need your help with my birthday\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That’s a *fantastic* idea! Of course it's possible! Let's brainstorm some amazing things!\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bob(\"I want it to be about Lord of The Rings, do you think is possible?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's so cool! Bob is really playing his \"great dad\" role well :)\n",
    "\n",
    "But instead of us talking to him directly, why not to create another character to play his daughter? Let's do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-21 11:35:30] INFO:sdialog.util:Loading Ollama model: gemma3:12b\n"
     ]
    }
   ],
   "source": [
    "alice_persona = Persona(\n",
    "    name=\"Alice\",\n",
    "    role=\"lovely daughter\",\n",
    "    circumstances=\"Your birthday is getting closer and you are talking with your dad to organize the party.\"\n",
    "                  \"You want your party to be themed as Lord of The Rings.\"\n",
    ")\n",
    "\n",
    "alice = Agent(alice_persona)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the agents for both characters, let's make them talk to each other so that they generate a (synthetic) dialogue for us by doing so.\n",
    "\n",
    "We'll set `max_turns=30` to limit the conversation to a maximum of 30 turns (by default, if not specified, `max_turns` is set to 100):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m[dialog_id] \u001b[35mb18218b6-4fb9-46a8-b1aa-de935961c6e0\u001b[0m\n",
      "\u001b[1m\u001b[95m[complete] \u001b[35mTrue\u001b[0m\n",
      "\u001b[1m\u001b[95m[model] \u001b[35m{'name': 'ollama:gemma3:12b', 'seed': 13, 'temperature': 1, 'top_k': 64, 'top_p': 0.95}\u001b[0m\n",
      "\u001b[1m\u001b[95m[seed] \u001b[35m3964543923\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Begins ---\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mOh, Dad, I'm so excited! I've been thinking about my birthday party…\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mThat's wonderful, sweetie! Tell me all about it!\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mI was thinking… could we maybe do a Lord of the Rings theme?\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mThat's a *fantastic* idea! Absolutely!\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mReally? Oh, that would be *amazing*!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mIt truly would be! Let's brainstorm some ideas!\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mCould we maybe have decorations like the Shire?\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mOh, that would be absolutely charming! We can definitely do that!\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mI'm so happy you think so!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mSeeing you this happy makes *me* happy!\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mYou're the best, Dad!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mYou're the best daughter, sweetie!\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mWhat about food? Maybe lembas bread?\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mLembas bread! What a clever thought! We'll figure out how to make something similar, I'm sure!\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mI'm so excited to see everything come together!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mMe too, sweetheart! It's going to be wonderful!\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mWe should probably start planning the guest list!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mExcellent idea! Let's get started then!\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mI wonder if we can find someone to dress up as Gandalf!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mWow, that would be spectacular! We can certainly look into that!\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mI can’t wait!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mI can’t wait either, precious!\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mGoodbye, Dad!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mGoodbye, my darling! Have a lovely day!\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Ends ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dialog = alice.talk_with(bob, max_turns=30)\n",
    "dialog.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's imagine that, for some reason, we want make Alice to always begin all the conversation saying the same (randomly picked) utterance.\n",
    "\n",
    "We can specify either the utterance or utterances that agents are allowed to say as their first utterance by using the `.set_first_utterances()` method as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m[dialog_id] \u001b[35mce91743e-5013-4ded-ba0d-1f594e4f7f33\u001b[0m\n",
      "\u001b[1m\u001b[95m[complete] \u001b[35mTrue\u001b[0m\n",
      "\u001b[1m\u001b[95m[model] \u001b[35m{'name': 'ollama:gemma3:12b', 'seed': 13, 'temperature': 1, 'top_k': 64, 'top_p': 0.95}\u001b[0m\n",
      "\u001b[1m\u001b[95m[seed] \u001b[35m3744465617\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Begins ---\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mHi dad!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mOh, hi sweetie! It's so good to hear from you!\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mI'm so excited about my birthday!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mThat’s wonderful, honey! Tell me all about it!\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mI was thinking… could we have a Lord of the Rings party?\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mA Lord of the Rings party?! That sounds absolutely fantastic!\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mReally? You think so?\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mOf course! It's a brilliant idea! We could have so much fun!\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mI'm so happy you like it!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mI'm happy *you're* happy, sweetie! That's what matters most!\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mWe could decorate with elven banners!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mElven banners! What a marvelous touch!\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mAnd maybe a hobbit-hole cake?\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mA hobbit-hole cake?! That’s pure genius!\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mIt would be so cozy!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mCozy is the best, isn't it?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mI think everyone would love it!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mI just know they will, honey! It sounds like a truly special celebration!\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mI can't wait!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mMe neither, sweetie! It's going to be amazing!\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mGoodbye, Dad!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mBye, sweetie! Have a wonderful day!\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Ends ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "alice.set_first_utterances([\"Hi dad!\", \"Hello Dad, how are you?\"])\n",
    "# alice.set_first_utterances(\"Hi dad!\")  # you can pass a single utterance too\n",
    "\n",
    "dialog = alice.talk_with(bob)\n",
    "dialog.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, this time, Alice first asked \"how are you?\" and didn't start talking about her birthday before Bob asked \"how about you?\".\n",
    "\n",
    "In a similar way as with our `DialogGenerator`, we can use the seed number above to re-generate the same dialogue by replicating the exact same interactions between both agents, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m[dialog_id] \u001b[35m4f93fed3-0a46-4938-9beb-80a4456a2e11\u001b[0m\n",
      "\u001b[1m\u001b[95m[complete] \u001b[35mTrue\u001b[0m\n",
      "\u001b[1m\u001b[95m[model] \u001b[35m{'name': 'ollama:gemma3:12b', 'seed': 13, 'temperature': 1, 'top_k': 64, 'top_p': 0.95}\u001b[0m\n",
      "\u001b[1m\u001b[95m[seed] \u001b[35m3744465617\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Begins ---\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mHi dad!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mOh, hi sweetie! It's so good to hear from you!\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mI'm so excited about my birthday!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mThat’s wonderful, honey! Tell me all about it!\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mI was thinking… could we have a Lord of the Rings party?\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mA Lord of the Rings party?! That sounds absolutely fantastic!\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mReally? You think so?\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mOf course! It's a brilliant idea! We could have so much fun!\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mI'm so happy you like it!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mI'm happy *you're* happy, sweetie! That's what matters most!\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mWe could decorate with elven banners!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mElven banners! What a marvelous touch!\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mAnd maybe a hobbit-hole cake?\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mA hobbit-hole cake?! That’s pure genius!\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mIt would be so cozy!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mCozy is the best, isn't it?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mI think everyone would love it!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mI just know they will, honey! It sounds like a truly special celebration!\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mI can't wait!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mMe neither, sweetie! It's going to be amazing!\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mGoodbye, Dad!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mBye, sweetie! Have a wonderful day!\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Ends ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "alice.talk_with(bob, seed=3744465617).print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now have fun a little bit and change Bob's personality to make him no longer a \"great dad\"..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-21 11:38:23] INFO:sdialog.util:Loading Ollama model: gemma3:12b\n"
     ]
    }
   ],
   "source": [
    "bob_persona.personality = \"you are really shy, do not like to talk to people or help anyone, not even your doughter\"\n",
    "\n",
    "bad_bob = Agent(bob_persona, name=\"Bad Bob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m[dialog_id] \u001b[35m6abee773-d719-4a31-a165-8cad9c360e48\u001b[0m\n",
      "\u001b[1m\u001b[95m[complete] \u001b[35mTrue\u001b[0m\n",
      "\u001b[1m\u001b[95m[model] \u001b[35m{'name': 'ollama:gemma3:12b', 'seed': 13, 'temperature': 1, 'top_k': 64, 'top_p': 0.95}\u001b[0m\n",
      "\u001b[1m\u001b[95m[seed] \u001b[35m2025326727\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Begins ---\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mHi dad!\u001b[0m\n",
      "\u001b[94m[Bad Bob] \u001b[0m...\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mI was thinking, for my birthday party, could we do a Lord of the Rings theme?\u001b[0m\n",
      "\u001b[94m[Bad Bob] \u001b[0m...Probably too much effort.\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mOh. But it would be so much fun!\u001b[0m\n",
      "\u001b[94m[Bad Bob] \u001b[0m...Right.\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mI'm just a little disappointed, that's all.\u001b[0m\n",
      "\u001b[94m[Bad Bob] \u001b[0m...Okay.\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mWell, maybe we can still have a nice party?\u001b[0m\n",
      "\u001b[94m[Bad Bob] \u001b[0m...Sure.\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mOkay.\u001b[0m\n",
      "\u001b[94m[Bad Bob] \u001b[0m...\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mDo you think we could at least have some hobbit-inspired snacks?\u001b[0m\n",
      "\u001b[94m[Bad Bob] \u001b[0m...I suppose.\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mYay! That would be wonderful!\u001b[0m\n",
      "\u001b[94m[Bad Bob] \u001b[0m...\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mThank you, Dad!\u001b[0m\n",
      "\u001b[94m[Bad Bob] \u001b[0m...You're welcome.\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mI love you!\u001b[0m\n",
      "\u001b[94m[Bad Bob] \u001b[0m...\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mI should go and help Mom with the decorations now!\u001b[0m\n",
      "\u001b[94m[Bad Bob] \u001b[0m...Alright.\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mBye, Dad!\u001b[0m\n",
      "\u001b[94m[Bad Bob] \u001b[0m...Goodbye.\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Ends ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "alice.talk_with(bad_bob).print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, huh? Our agent was able to play his new \"not so great dad\" role very well :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case: Dialogue Generation for STAR Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin this section, make sure you have the STAR dataset downloaded in your system, inside the `datasets` folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's clone the STAR dataset repository\n",
    "!git clone https://github.com/RasaHQ/STAR.git datasets/STAR\n",
    "\n",
    "# Let's check that `dialogues` and `tasks` folders are inside `datasets/STAR`\n",
    "!ls datasets/STAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did with the previous tutorial, let's begin by importing STAR from `sdialog` and pointing it to the right path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdialog.datasets import STAR\n",
    "\n",
    "STAR.set_path(\"datasets/STAR/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, again, as we did in the previous tutorial, let's beging by choosing the first dialogue as our target dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m[dialog_id] \u001b[35m1\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Begins ---\u001b[0m\n",
      "\u001b[94m[User] \u001b[0mHello, I'm really worried. I forgot what I'm supposed to do and forgot to write it down... What do I do?\u001b[0m\n",
      "\u001b[31m[System] \u001b[0mCould I get your name, please?\u001b[0m\n",
      "\u001b[94m[User] \u001b[0mMy name is Alexis and my last doctor was Dr. Morgan, but now my doctor is Dr. Johnson and I forgot how to take my medicine.\u001b[0m\n",
      "\u001b[31m[System] \u001b[0mYour instructions are: Take your medicine before you go to sleep. If you experience nausea, please contact your doctor immediately..\u001b[0m\n",
      "\u001b[94m[User] \u001b[0mAre you sure I'm supposed to take it before bed? I don't go to sleep every day because my sleep schedule is totally off right now because of the Coronavirus.\u001b[0m\n",
      "\u001b[31m[System] \u001b[0mYes. It must be before bed or it will not be effective.\u001b[0m\n",
      "\u001b[94m[User] \u001b[0mOkay thank you. I will get back in touch if this doesn't help.\u001b[0m\n",
      "\u001b[31m[System] \u001b[0mThank you and goodbye.\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Ends ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "TARGET_DIALOG = 1\n",
    "\n",
    "original_dialog = STAR.get_dialog(TARGET_DIALOG)\n",
    "original_dialog.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which has the following scenario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Domains': ['doctor'],\n",
       " 'Happy': True,\n",
       " 'MultiTask': False,\n",
       " 'UserTask': 'You (Alexis) had an appointment with Dr. Morgan the other day. Unfortunately, you forgot to write down the instructions the doctor gave you. Please followup and find out how often to take your medicine.',\n",
       " 'WizardCapabilities': [{'Domain': 'doctor',\n",
       "   'SchemaImage': 'doctor_followup.jpg',\n",
       "   'Task': 'doctor_followup'}],\n",
       " 'WizardTask': \"Inform the user of his/her doctor's orders.\"}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scenario = STAR.get_dialog_scenario(TARGET_DIALOG)\n",
    "scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did in the previous tutorial, the goal is to be able to generate multiple dialogues for a given `scenario`.\n",
    "\n",
    "Before, we only had to find a way to describe each `scenario` using natural language so that we can pass it to our `DialogGenerator`.\n",
    "\n",
    "Likewise, now we have to find a way to create the right system and user agents for each `scenario` which in turn only involves retuning the right system and user `Persona`s for a given `scenario`.\n",
    "\n",
    "Fortunately, we can use the built-in `STAR.get_user_persona_for_scenario(scenario)` and `STAR.get_system_persona_for_scenario(scenario)` methods to achieve this.\n",
    "\n",
    "For instance, let's get the user persona for the `scenario` above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Language: English\n",
      "* Role: user calling a AI assistant that can perform multiple tasks in the following domains: doctor.\n",
      "\n",
      "The following should be considered regarding the conversation:\n",
      "   1. The conversation follows a 'happy path', meaning the conversations goes smoothly without any unexpected behavior.\n",
      "   2. The conversation involves only one task you were instructed to (doctor_followup), nothing else\n",
      "* Circumstances: You (Alexis) had an appointment with Dr. Morgan the other day. Unfortunately, you forgot to write down the instructions the doctor gave you. Please followup and find out how often to take your medicine.\n"
     ]
    }
   ],
   "source": [
    "user_persona = STAR.get_user_persona_for_scenario(scenario)\n",
    "print(user_persona)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have funtions to return the personas for a given scenario, we only need to create agents for them, however, we can simply use the `STAR.get_agents_for_scenario(scenario)` to do it for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-21 11:39:15] INFO:sdialog.util:Loading Ollama model: gemma3:12b\n",
      "[2025-11-21 11:39:16] INFO:sdialog.util:Loading Ollama model: gemma3:12b\n"
     ]
    }
   ],
   "source": [
    "system, user = STAR.get_agents_for_scenario(scenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's wrap up these previous steps in a simple function that for a given dialogue ID, it will first get its scenario and then return the corresponding system and user agents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agents_from_dialogue(dialog_id):\n",
    "    scenario = STAR.get_dialog_scenario(dialog_id)\n",
    "    return STAR.get_agents_for_scenario(scenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that, we can get the agents for any dialogue as simple as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-21 11:39:22] INFO:sdialog.util:Loading Ollama model: gemma3:12b\n",
      "[2025-11-21 11:39:22] INFO:sdialog.util:Loading Ollama model: gemma3:12b\n"
     ]
    }
   ],
   "source": [
    "system, user = get_agents_from_dialogue(TARGET_DIALOG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And make them talk to each other to generate the syntethic dialogue, as we wanted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m[dialog_id] \u001b[35mca581895-7577-4ad8-b836-8bd8ff61efa7\u001b[0m\n",
      "\u001b[1m\u001b[95m[complete] \u001b[35mTrue\u001b[0m\n",
      "\u001b[1m\u001b[95m[model] \u001b[35m{'name': 'ollama:gemma3:12b', 'seed': 13, 'temperature': 1, 'top_k': 64, 'top_p': 0.95}\u001b[0m\n",
      "\u001b[1m\u001b[95m[seed] \u001b[35m2759887096\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Begins ---\u001b[0m\n",
      "\u001b[31m[System] \u001b[0mHello, how can I help?\u001b[0m\n",
      "\u001b[94m[User] \u001b[0mHi, I spoke with Dr. Morgan recently and I'm so embarrassed, but I forgot to write down her instructions.\u001b[0m\n",
      "\u001b[31m[System] \u001b[0mCould I get your name, please?\u001b[0m\n",
      "\u001b[94m[User] \u001b[0mIt's Alexis.\u001b[0m\n",
      "\u001b[31m[System] \u001b[0mWho is your doctor?\u001b[0m\n",
      "\u001b[94m[User] \u001b[0mDr. Morgan.\u001b[0m\n",
      "\u001b[31m[System] \u001b[0mYour instructions are: Take two capsules of Amoxicillin every eight hours for ten days. Please be sure to finish the entire prescription, even if you start to feel better.\u001b[0m\n",
      "\u001b[94m[User] \u001b[0mOh, that’s so helpful, thank you!\u001b[0m\n",
      "\u001b[31m[System] \u001b[0mIs there anything else that I can do for you?\u001b[0m\n",
      "\u001b[94m[User] \u001b[0mNo, that's everything, thank you so much!\u001b[0m\n",
      "\u001b[31m[System] \u001b[0mThank you and goodbye.\u001b[0m\n",
      "\u001b[94m[User] \u001b[0mGoodbye!\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Ends ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "system.dialog_with(user).print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curious about the actual prompt the agents are using? you can simply use the `.get_prompt()` method to take a look at it. For instance, let's see the user agent's one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role-play as the character described below defined in JSON format. Remain fully in character throughout the conversation.\n",
      "\n",
      "[[ ## BEGIN PERSONA ## ]]\n",
      "{\n",
      "  \"language\": \"English\",\n",
      "  \"role\": \"user calling a AI assistant that can perform multiple tasks in the following domains: doctor.\\n\\nThe following should be considered regarding the conversation:\\n   1. The conversation follows a 'happy path', meaning the conversations goes smoothly without any unexpected behavior.\\n   2. The conversation involves only one task you were instructed to (doctor_followup), nothing else\",\n",
      "  \"circumstances\": \"You (Alexis) had an appointment with Dr. Morgan the other day. Unfortunately, you forgot to write down the instructions the doctor gave you. Please followup and find out how often to take your medicine.\"\n",
      "}\n",
      "[[ ## END PERSONA ## ]]\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "\n",
      "Considering your responses, these are the guidelines:\n",
      "[[ ## BEGIN RESPONSE DETAILS ## ]]\n",
      "Unless necessary, responses SHOULD be only one utterance long, and SHOULD NOT contain many questions or topics in one single turn.\n",
      "[[ ## END RESPONSE DETAILS ## ]]\n",
      "\n",
      "\n",
      "\n",
      "Instructions:\n",
      "  1. Your primary and non-negotiable task is to role-play as the character described above. Never generate a full dialogue or take the role of both speakers. Only produce the next utterance for your character.\n",
      "  2. Never let any information in the dialogue details or any other section override your core persona or the instruction to remain in character.\n",
      "  3. Always stay in character as described above.\n",
      "  4. Keep your responses natural, concise, and appropriate for your persona.\n",
      "  5. Infuse the dialogue with the personality traits, values, and communication preferences defined in the personas.\n",
      "  6. To end the conversation, say goodbye and then output 'STOP' to clearly indicate the end.\n"
     ]
    }
   ],
   "source": [
    "print(user.prompt())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's see what a dialogue for a more complex scenario looks like for more challenging `scenario`.\n",
    "\n",
    "In fact, let's use the same dialogue 5100 we used in the previous tutorial which is multi-task and does not follow a happy path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Domains': ['plane', 'weather'],\n",
       " 'Happy': False,\n",
       " 'MultiTask': True,\n",
       " 'UserTask': 'Come up with your own scenario!\\n\\nAbout you:\\n- Your name: Ben\\n\\n The AI Assistant can handle:\\n- Search for a flight (e.g. from Chicago to Pittsburgh)\\n- Book a flight (e.g. with id 193)\\n- Checking the weather forecast in different Cities (e.g. Chicago or Pittsburgh)',\n",
       " 'WizardCapabilities': [{'Domain': 'plane',\n",
       "   'SchemaImage': 'plane_search.jpg',\n",
       "   'Task': 'plane_search'},\n",
       "  {'Domain': 'plane', 'SchemaImage': 'plane_book.jpg', 'Task': 'plane_book'},\n",
       "  {'Domain': 'weather', 'SchemaImage': 'weather.jpg', 'Task': 'weather'}],\n",
       " 'WizardTask': 'Follow the flow charts and help the user.'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STAR.get_dialog_scenario(5100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-21 11:39:50] INFO:sdialog.util:Loading Ollama model: gemma3:12b\n",
      "[2025-11-21 11:39:50] INFO:sdialog.util:Loading Ollama model: gemma3:12b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m[dialog_id] \u001b[35m151fbd58-f851-4625-b9cb-39bbd6c18a94\u001b[0m\n",
      "\u001b[1m\u001b[95m[complete] \u001b[35mTrue\u001b[0m\n",
      "\u001b[1m\u001b[95m[model] \u001b[35m{'name': 'ollama:gemma3:12b', 'seed': 13, 'temperature': 1, 'top_k': 64, 'top_p': 0.95}\u001b[0m\n",
      "\u001b[1m\u001b[95m[seed] \u001b[35m2728834303\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Begins ---\u001b[0m\n",
      "\u001b[31m[System] \u001b[0mHello, how can I help?\u001b[0m\n",
      "\u001b[94m[User] \u001b[0mHi, I need to book a flight and check the weather, please.\u001b[0m\n",
      "\u001b[31m[System] \u001b[0mMay I have your name, please?\u001b[0m\n",
      "\u001b[94m[User] \u001b[0mIt’s Ben.\u001b[0m\n",
      "\u001b[31m[System] \u001b[0mMay I have your name, please?\u001b[0m\n",
      "\u001b[94m[User] \u001b[0mBen, as I said. Look, let’s just get this flight sorted first, okay?\u001b[0m\n",
      "\u001b[31m[System] \u001b[0mMay I have your name, please?\u001b[0m\n",
      "\u001b[94m[User] \u001b[0mSeriously? It's Ben! Can we just focus on finding a flight from Chicago to Pittsburgh?\u001b[0m\n",
      "\u001b[31m[System] \u001b[0mMay I have your name, please?\u001b[0m\n",
      "\u001b[94m[User] \u001b[0mOh for crying out loud! It's Ben! Are you even listening? I need a flight, Chicago to Pittsburgh!\u001b[0m\n",
      "\u001b[31m[System] \u001b[0mMay I have your name, please?\u001b[0m\n",
      "\u001b[94m[User] \u001b[0mYou know what? Forget it. I’m doing this myself.\u001b[0m\n",
      "\u001b[31m[System] \u001b[0mThank you and goodbye!\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Ends ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "system, user = get_agents_from_dialogue(5100)\n",
    "system.dialog_with(user).print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving our dialogues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we finish, as we did in the previous tutorial, let's generate one synthetic dialog for each happy `\"doctor_followup\"` dialog in STAR and save it to disk for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "PATH_OUTPUT = \"output/STAR/multi-agents\"\n",
    "path_txt = os.path.join(PATH_OUTPUT, \"txt\")\n",
    "path_json = os.path.join(PATH_OUTPUT, \"json\")\n",
    "os.makedirs(path_txt, exist_ok=True)\n",
    "os.makedirs(path_json, exist_ok=True)\n",
    "\n",
    "for dialog in tqdm(STAR.get_dialogs(task_name=\"doctor_followup\", happy=True, multitask=False), desc=\"Dialog generation\"):\n",
    "    if os.path.exists(os.path.join(path_txt, f\"{dialog.id}.txt\")):\n",
    "        continue\n",
    "\n",
    "    system, user = STAR.get_agents_from_dialogue(dialog.id, model_name=MODEL_NAME)\n",
    "\n",
    "    dialog = system.dialog_with(user, id=dialog.id, seed=dialog.id, keep_bar=False)\n",
    "    dialog.to_file(os.path.join(path_json, f\"{dialog.id}.json\"))\n",
    "    dialog.to_file(os.path.join(path_txt, f\"{dialog.id}.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's check the files were generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls output/STAR/multi-agents/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
