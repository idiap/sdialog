{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Dialogue Generation\n",
    "\n",
    "<p align=\"right\" style=\"margin-right: 8px;\">\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/idiap/sdialog/blob/main/tutorials/00_overview/1.single_llm_full_generation.ipynb\">\n",
    "        <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first check if our environment is all set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in Jupyter Notebook\n"
     ]
    }
   ],
   "source": [
    "# Setup the environment depending on weather we are running in Google Colab or Jupyter Notebook\n",
    "from IPython import get_ipython\n",
    "\n",
    "if \"google.colab\" in str(get_ipython()):\n",
    "    print(\"Running on CoLab\")\n",
    "    # Downloading only the \"output\" directory from the repository\n",
    "    !git init .\n",
    "    !git remote add -f origin https://github.com/Play-Your-Part/tutorials.git\n",
    "    !git config core.sparseCheckout true\n",
    "    !echo \"output\" >> .git/info/sparse-checkout\n",
    "    !git pull origin main\n",
    "\n",
    "    # Installing Ollama\n",
    "    !curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "    # Installing sdialog\n",
    "    !git clone https://github.com/idiap/sdialog.git\n",
    "    %cd sdialog\n",
    "    %pip install -e .\n",
    "    %cd ..\n",
    "else:\n",
    "    print(\"Running in Jupyter Notebook\")\n",
    "    # Little hack to avoid the \"OSError: Background processes not supported.\" error in Jupyter notebooks\"\n",
    "    import os\n",
    "    get_ipython().system = os.system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \u26a0\ufe0f If you're using **Colab**, please, **restart the runtime** once everything above is installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ollama Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the ollama server first, as a background process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!OLLAMA_KEEP_ALIVE=-1 ollama serve > /dev/null 2>&1 &\n",
    "!sleep 10  # Let's wait a bit for the server to start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change the default sdialog model to `qwen2.5:14b`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sdialog\n",
    "\n",
    "sdialog.config.llm(\"qwen2.5:14b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Output (Dialogue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin by defining the JSON objects that we will use to represent the generated dialogues. For now this object will have only three fields: `\"model\"`, `\"seed\"`, `\"scenario\"`, and `\"dialog\"` to store the name of the model and the seed used to generate the dialogue, as well as the scenario associated to the dialogue and the dialogue itself, respectively. More preciselly, the `\"dialog\"` field will contain the list of turns of the conversation in order, with the speaker name and the corresponding utterances. As shown in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_dialogue = {\n",
    "    \"model\": \"qwen2.5:14b\",  # the model used to generate the dialogue\n",
    "    \"seed\": 123,  # the seed used to generated\n",
    "    \"scenario\": \"short hello and good bye conversation\",  # the scenario used to generated the dialogue\n",
    "    \"turns\": [\n",
    "        {\"speaker\": \"Alice\", \"text\": \"Hey Bob!\"},\n",
    "        {\"speaker\": \"Bob\", \"text\": \"Hey Alice!\"},\n",
    "        {\"speaker\": \"Alice\", \"text\": \"Bye Bob!\"},\n",
    "        {\"speaker\": \"Bob\", \"text\": \"Bye bye!\"},\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `pydantic` to properly define our `Dialogue` type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List, Union, Optional\n",
    "\n",
    "class Turn(BaseModel):\n",
    "    speaker: str\n",
    "    text: str\n",
    "\n",
    "class Dialog(BaseModel):\n",
    "    model: str  # the model used to generate the dialogue\n",
    "    seed: int  # the seed used to generated\n",
    "    scenario: Optional[Union[dict, str]] = None  # the scenario used to generated the dialogue\n",
    "    turns: List[Turn]  # the list of turns of the conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a Python `pydantic` class to formally represent our dialogues is quite useful, we can convert any JSON dialogue to our `Dialog` class as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dialog(model='qwen2.5:14b', seed=123, scenario='short hello and good bye conversation', turns=[Turn(speaker='Alice', text='Hey Bob!'), Turn(speaker='Bob', text='Hey Alice!'), Turn(speaker='Alice', text='Bye Bob!'), Turn(speaker='Bob', text='Bye bye!')])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dialogue = Dialog.model_validate(example_dialogue)\n",
    "my_dialogue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or the opposite, convert our `Dialog`s to a `dict` or a JSON as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'qwen2.5:14b',\n",
       " 'seed': 123,\n",
       " 'scenario': 'short hello and good bye conversation',\n",
       " 'turns': [{'speaker': 'Alice', 'text': 'Hey Bob!'},\n",
       "  {'speaker': 'Bob', 'text': 'Hey Alice!'},\n",
       "  {'speaker': 'Alice', 'text': 'Bye Bob!'},\n",
       "  {'speaker': 'Bob', 'text': 'Bye bye!'}]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dialogue.model_dump()  # a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"model\": \"qwen2.5:14b\",\n",
      "  \"seed\": 123,\n",
      "  \"scenario\": \"short hello and good bye conversation\",\n",
      "  \"turns\": [\n",
      "    {\n",
      "      \"speaker\": \"Alice\",\n",
      "      \"text\": \"Hey Bob!\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Bob\",\n",
      "      \"text\": \"Hey Alice!\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Alice\",\n",
      "      \"text\": \"Bye Bob!\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Bob\",\n",
      "      \"text\": \"Bye bye!\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "my_dialogue_json = my_dialogue.model_dump_json(indent=2)  # a string containing the dialog as a JSON object\n",
    "print(my_dialogue_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, of course, create a new `Dialog` from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dialog(model='qwen2.5:14b', seed=123, scenario=None, turns=[Turn(speaker='Alice', text='Hi :)'), Turn(speaker='Bob', text='Bye! :(')])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dialog(\n",
    "    model=\"qwen2.5:14b\",\n",
    "    seed=123,\n",
    "    turns=[\n",
    "        Turn(speaker=\"Alice\", text=\"Hi :)\"),\n",
    "        Turn(speaker=\"Bob\", text=\"Bye! :(\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternativelly, we can use the built-in `Dialog` class from `sdialog`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dialog(version='0.0.2+2b706d22e2adfdd6e9985e45f97ec24d1c2907ce', timestamp='2025-07-09T13:37:12Z', model='qwen2.5:14b', seed=123, id=None, parentId=None, complete=None, personas=None, scenario='short hello and good bye conversation', turns=[Turn(speaker='Alice', text='Hey Bob!'), Turn(speaker='Bob', text='Hey Alice!'), Turn(speaker='Alice', text='Bye Bob!'), Turn(speaker='Bob', text='Bye bye!')], events=None, notes=None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sdialog import Dialog\n",
    "\n",
    "my_dialog = Dialog.model_validate(example_dialogue)\n",
    "my_dialog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which besides providing the exact same functionalities, among other things, allow us to:\n",
    "\n",
    "- Pretty print the dialogue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m[model] \u001b[35mqwen2.5:14b\u001b[0m\n",
      "\u001b[1m\u001b[95m[seed] \u001b[35m123\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Begins ---\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mHey Bob!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mHey Alice!\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mBye Bob!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mBye bye!\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Ends ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "my_dialog.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Print it in a vanilla textual form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice: Hey Bob!\n",
      "Bob: Hey Alice!\n",
      "Alice: Bye Bob!\n",
      "Bob: Bye bye!\n"
     ]
    }
   ],
   "source": [
    "print(my_dialog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Save it to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# either as a JSON object\n",
    "my_dialog.to_file(\"output/my_dialogue.json\")\n",
    "\n",
    "# or a txt file\n",
    "my_dialog.to_file(\"output/my_dialogue.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_(check created files [`output/my_dialogue.json`](output/my_dialogue.json) and [`output/my_dialogue.txt`](output/my_dialogue.txt))_\n",
    "\n",
    "- Load a dialogue from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m[model] \u001b[35mqwen2.5:14b\u001b[0m\n",
      "\u001b[1m\u001b[95m[seed] \u001b[35m123\u001b[0m\n",
      "\u001b[1m\u001b[95m[scenario] \u001b[35m\u001b[0m\n",
      "\u001b[35mshort hello and good bye conversation\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Begins ---\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mHey Bob!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mHey Alice!\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mBye Bob!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mBye bye!\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Ends ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "my_dialog = Dialog.from_file(\"output/my_dialogue.json\")\n",
    "my_dialog.print(scenario=True)  # `scenario=True` to also print the metadata stored in scenario field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[35m--- Dialogue Begins ---\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mHey Bob!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mHey Alice!\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mBye Bob!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mBye bye!\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Ends ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "my_dialog = Dialog.from_file(\"output/my_dialogue.txt\")\n",
    "my_dialog.print(scenario=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Or simple things like quickly know how long a dialogue is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_dialog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to begin working on synthetic `Dialog` (;)) generation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dialogue Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description-driven Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `sdialog`'s built-in `DialogueGenerator` class that we can instantiate using descriptions to generate our dialogues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdialog.generators import DialogGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which takes the following arguments as input:\n",
    "- `model` model name to use (any model tag from [ollama hub](https://ollama.com/library)).\n",
    "- `dialogue_details` the details about the desired dialogue.\n",
    "- `output_format` the output format as a `pydantic` class or JSON scheme, as we did above (`LLMDialogOutput` by default).\n",
    "- `scenario` an optional metadata field that describes the scenario used to generated dialogue.\n",
    "\n",
    "For instance, let's create an instance of `DialogGenerator` to generate conversations between Bob and Alice about her birthday:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialog_generator = DialogGenerator(\n",
    "    dialogue_details=\"The conversation is between a dad (Bob) and his doughter (Alice). \"\n",
    "                     \"Her birthday is coming up and she wants to throw a Star Wars themed party.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can use the build-in `.generate()` method to generate conversations for such instance:\n",
    "\n",
    "_(each time you run the code below, a different dialogue will be generated)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-09 15:48:06] INFO:sdialog.generators:Prompt used: [{'type': 'system', 'data': {'content': 'You are a highly capable AI assistant skilled at writing natural, multi-turn conversations by role-playing different speakers. Generate a complete, realistic dialogue from start (greetings) to finish (goodbye), ensuring each speaker remains consistent and the flow is natural and engaging.\\n\\n', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'system', 'name': None, 'id': None}}, {'type': 'human', 'data': {'content': 'The conversation is between a dad (Bob) and his doughter (Alice). Her birthday is coming up and she wants to throw a Star Wars themed party.', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': None, 'example': False}}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m[dialog_id] \u001b[35m1752068886722\u001b[0m\n",
      "\u001b[1m\u001b[95m[model] \u001b[35mmodel='qwen2.5:14b' temperature=0.8 seed=13 format={'$defs': {'Turn': {'description': 'Represents a single turn in a dialogue.\\n\\n:ivar speaker: The name or role of the speaker.\\n:vartype speaker: Optional[str]\\n:ivar text: The utterance text for this turn.\\n:vartype text: str', 'properties': {'speaker': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Speaker'}, 'text': {'title': 'Text', 'type': 'string'}}, 'required': ['speaker', 'text'], 'title': 'Turn', 'type': 'object'}}, 'description': 'Pydantic model for LLM-generated dialogue output.\\n\\n:ivar dialog: List of dialogue turns.\\n:vartype dialog: List[Turn]', 'properties': {'dialog': {'items': {'$ref': '#/$defs/Turn'}, 'title': 'Dialog', 'type': 'array'}}, 'required': ['dialog'], 'title': 'LLMDialogOutput', 'type': 'object'}\u001b[0m\n",
      "\u001b[1m\u001b[95m[seed] \u001b[35m2071202074\u001b[0m\n",
      "\u001b[1m\u001b[95m[scenario] \u001b[35m\u001b[0m\n",
      "\u001b[35mThe conversation is between a dad (Bob) and his doughter (Alice). Her birthday is coming up and she wants to throw a Star Wars themed party.\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Begins ---\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mHey Alice, how's your day going?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mHi Dad! It's going well but I'm a bit nervous about my birthday party next weekend. I want to throw a Star Wars themed one and I need some help.\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mOh, that sounds exciting! What kind of help do you think you'll need? Decorations, snacks, invites?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mWell, all of those actually. I was thinking we could make some DIY decorations and maybe have a lightsaber duel game or something fun like that.\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mDIY decorations? That sounds like a lot of fun! Do you want to start by making posters with your favorite characters or are there specific things you'd like help with?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mI was thinking we could make some cool signs for the door and maybe have some star wars food. I saw something online about Death Star cupcakes, they look amazing but I'm not sure how to do them.\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mThat sounds great! We can find a simple recipe for those cupcakes. And we could make Darth Vader cake pops too if you'd like. What else would be on your party menu?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mI was hoping to have some of the snacks that Obi-Wan had when he met Luke in the movie, do you know what those were? Maybe we could recreate them.\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mThat's a great idea! In the scene, Obi-Wan offers Luke some blue milk and other snacks. We can make something similar using some fruit juices and maybe some cookies or crackers to represent the snacks in the movie.\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mAwesome! Dad, you're really good at this. I'm so happy we could do it together!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mIt's my pleasure, sweetie. We'll have a blast putting everything together and creating the best Star Wars party ever. Let's start planning what to buy for tomorrow.\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Ends ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dialog = dialog_generator.generate()\n",
    "dialog.print(scenario=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now change the description, now the party has to be about Lord of the Rings, not Star Wars. To do this we can simply pass the new description when calling the generate method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-09 15:48:20] INFO:sdialog.generators:Prompt used: [{'type': 'system', 'data': {'content': 'You are a highly capable AI assistant skilled at writing natural, multi-turn conversations by role-playing different speakers. Generate a complete, realistic dialogue from start (greetings) to finish (goodbye), ensuring each speaker remains consistent and the flow is natural and engaging.\\n\\n', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'system', 'name': None, 'id': None}}, {'type': 'human', 'data': {'content': 'The conversation is between a dad (Bob) and his doughter (Alice). Her birthday is coming up and she wants to throw a Lord of the Rings themed party.', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': None, 'example': False}}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m[dialog_id] \u001b[35m1752068900414\u001b[0m\n",
      "\u001b[1m\u001b[95m[model] \u001b[35mmodel='qwen2.5:14b' temperature=0.8 seed=13 format={'$defs': {'Turn': {'description': 'Represents a single turn in a dialogue.\\n\\n:ivar speaker: The name or role of the speaker.\\n:vartype speaker: Optional[str]\\n:ivar text: The utterance text for this turn.\\n:vartype text: str', 'properties': {'speaker': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Speaker'}, 'text': {'title': 'Text', 'type': 'string'}}, 'required': ['speaker', 'text'], 'title': 'Turn', 'type': 'object'}}, 'description': 'Pydantic model for LLM-generated dialogue output.\\n\\n:ivar dialog: List of dialogue turns.\\n:vartype dialog: List[Turn]', 'properties': {'dialog': {'items': {'$ref': '#/$defs/Turn'}, 'title': 'Dialog', 'type': 'array'}}, 'required': ['dialog'], 'title': 'LLMDialogOutput', 'type': 'object'}\u001b[0m\n",
      "\u001b[1m\u001b[95m[seed] \u001b[35m2167019297\u001b[0m\n",
      "\u001b[1m\u001b[95m[scenario] \u001b[35m\u001b[0m\n",
      "\u001b[35mThe conversation is between a dad (Bob) and his doughter (Alice). Her birthday is coming up and she wants to throw a Star Wars themed party.\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Begins ---\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mHey Alice, what's on your mind today?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mHi Dad! I've been thinking a lot about my birthday coming up.\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mThat's right, it will be here before we know it. What do you have in mind?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mI was hoping to throw a Lord of the Rings themed party! Everyone loves that movie series.\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mThat sounds fun, Alice! Have you thought about how you want to set it up?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mYeah, I'd love some decorations and maybe even a special menu. Plus we could all dress up as our favorite characters.\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mThat sounds like a great idea! What kind of games do you think would be fun to play?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mWe could have a trivia quiz about the movies and maybe an obstacle course inspired by some scenes.\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mThose sound like awesome ideas! I can help you with planning. Who are you inviting?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mI want to invite all my friends from school and maybe a few of your work colleagues who enjoy it too.\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mSounds great, let's make sure we have enough food and drinks for everyone. And don't forget about the cake!\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mI'm so excited! Thanks for being cool with it, Dad.\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mNo problem, sweetie. It's going to be a fantastic party! Let's start planning in more detail this weekend?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mThat would be awesome! Thanks again, Dad.\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mYou're welcome. I'm looking forward to it already!\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Ends ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dialog = dialog_generator.generate(\n",
    "    \"The conversation is between a dad (Bob) and his doughter (Alice). \"\n",
    "    \"Her birthday is coming up and she wants to throw a Lord of the Rings themed party.\"\n",
    ")\n",
    "dialog.print(scenario=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the `seed` number above to re-generate the exact same dialogue each time as an argument of `generate()`, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-09 15:50:03] INFO:sdialog.generators:Prompt used: [{'type': 'system', 'data': {'content': 'You are a highly capable AI assistant skilled at writing natural, multi-turn conversations by role-playing different speakers. Generate a complete, realistic dialogue from start (greetings) to finish (goodbye), ensuring each speaker remains consistent and the flow is natural and engaging.\\n\\n', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'system', 'name': None, 'id': None}}, {'type': 'human', 'data': {'content': 'The conversation is between a dad (Bob) and his doughter (Alice). Her birthday is coming up and she wants to throw a Lord of the Rings themed party.', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': None, 'example': False}}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m[dialog_id] \u001b[35m1752069003606\u001b[0m\n",
      "\u001b[1m\u001b[95m[model] \u001b[35mmodel='qwen2.5:14b' temperature=0.8 seed=13 format={'$defs': {'Turn': {'description': 'Represents a single turn in a dialogue.\\n\\n:ivar speaker: The name or role of the speaker.\\n:vartype speaker: Optional[str]\\n:ivar text: The utterance text for this turn.\\n:vartype text: str', 'properties': {'speaker': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Speaker'}, 'text': {'title': 'Text', 'type': 'string'}}, 'required': ['speaker', 'text'], 'title': 'Turn', 'type': 'object'}}, 'description': 'Pydantic model for LLM-generated dialogue output.\\n\\n:ivar dialog: List of dialogue turns.\\n:vartype dialog: List[Turn]', 'properties': {'dialog': {'items': {'$ref': '#/$defs/Turn'}, 'title': 'Dialog', 'type': 'array'}}, 'required': ['dialog'], 'title': 'LLMDialogOutput', 'type': 'object'}\u001b[0m\n",
      "\u001b[1m\u001b[95m[seed] \u001b[35m2167019297\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Begins ---\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mHey Alice, what's on your mind today?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mHi Dad! I've been thinking a lot about my birthday coming up.\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mThat's right, it will be here before we know it. What do you have in mind?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mI was hoping to throw a Lord of the Rings themed party! Everyone loves that movie series.\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mThat sounds fun, Alice! Have you thought about how you want to set it up?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mYeah, I'd love some decorations and maybe even a special menu. Plus we could all dress up as our favorite characters.\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mThat sounds like a great idea! What kind of games do you think would be fun to play?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mWe could have a trivia quiz about the movies and maybe an obstacle course inspired by some scenes.\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mThose sound like awesome ideas! I can help you with planning. Who are you inviting?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mI want to invite all my friends from school and maybe a few of your work colleagues who enjoy it too.\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mSounds great, let's make sure we have enough food and drinks for everyone. And don't forget about the cake!\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mI'm so excited! Thanks for being cool with it, Dad.\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mNo problem, sweetie. It's going to be a fantastic party! Let's start planning in more detail this weekend?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mThat would be awesome! Thanks again, Dad.\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mYou're welcome. I'm looking forward to it already!\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Ends ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dialog_generator.generate(\n",
    "        \"The conversation is between a dad (Bob) and his doughter (Alice). \"\n",
    "        \"Her birthday is coming up and she wants to throw a Lord of the Rings themed party.\",\n",
    "        seed=2167019297\n",
    ").print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Role-Playing-based Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal here will be to have to LLM to generate the dialogues by role-playing the different charecters.\n",
    "\n",
    "Each character will be fully defined by its persona, so, the same way started this tutorial by defining what a \"synthetic `Dialog`\" will actually be, we should now define our `Persona`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, the `sdialog` contains a `BasePersona` that we can import to create our own custom persona classes, let's import it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdialog.personas import BasePersona"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's define our concrete `Persona` class now by specifying some useful attributes like a name, role, background, etc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Persona(BasePersona):\n",
    "    name: str = \"\"\n",
    "    role: str = \"\"\n",
    "    background: str = \"\"\n",
    "    personality: str = \"\"\n",
    "    circumstances: str = \"\"\n",
    "    rules: str = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create/instantiate any `Persona` we want for our characters. Let's create our Bob and Alice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "bob_persona = Persona(\n",
    "        name=\"Bob\",\n",
    "        role=\"great dad\",\n",
    "        circumstances=\"Your daughter will talk to you\",\n",
    "        background=\"Computer Science PhD.\",\n",
    "        personality=\"an extremely happy person that likes to help people\",\n",
    ")\n",
    "\n",
    "alice_persona = Persona(\n",
    "    name=\"Alice\",\n",
    "    role=\"lovely daughter\",\n",
    "    circumstances=\"Your birthday is getting closer and you are talking with your dad to organize the party.\"\n",
    "                  \"You want your party to be themed as Lord of The Rings.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we print `bob` we will see that it is automatically converted to natural language description, which is usefull when we want to create the actual prompt for our LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Name: Bob\n",
      "* Role: great dad\n",
      "* Background: Computer Science PhD.\n",
      "* Personality: an extremely happy person that likes to help people\n",
      "* Circumstances: Your daughter will talk to you\n"
     ]
    }
   ],
   "source": [
    "print(bob_persona)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case we are working with a really complex persona and this default description is not good for your needs, you can overwrite it by defining your own `description()` method as in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your awesome name is Bob and your awesome role is being a great dad\n"
     ]
    }
   ],
   "source": [
    "class PersonaCustom(BasePersona):\n",
    "    name: str = \"\"\n",
    "    role: str = \"\"\n",
    "\n",
    "    def description(self):\n",
    "        return f\"Your awesome name is {self.name} and your awesome role is being a {self.role}\"\n",
    "\n",
    "awesome_bob = PersonaCustom(\n",
    "        name=\"Bob\",\n",
    "        role=\"great dad\"\n",
    ")\n",
    "\n",
    "# Let's print \"awesome_bob\" persona\n",
    "print(awesome_bob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we now know how to create personas, let's move to the fun part which is actually creating the actual generator.\n",
    "\n",
    "Fortunatelly, we can simply use `sdialog`'s built-in `PersonaDialogGenerator` class to generate our persona-based dialogues as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-09 15:50:46] INFO:sdialog.generators:Prompt used: [{'type': 'system', 'data': {'content': 'You are a highly capable AI assistant skilled at writing natural, multi-turn conversations by role-playing different speakers. Generate a complete, realistic dialogue from start (greetings) to finish (goodbye), ensuring each speaker remains consistent and the flow is natural and engaging.\\n\\n', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'system', 'name': None, 'id': None}}, {'type': 'human', 'data': {'content': '# Improved two-persona dialogue prompt\\nRole-play a conversation between the following two characters. Each character is defined by their persona below in JSON format. Remain in character for both roles at all times.\\n\\n[[ ## BEGIN FIRST PERSONA ## ]]\\n{\\n  \"name\": \"Bob\",\\n  \"role\": \"great dad\",\\n  \"background\": \"Computer Science PhD.\",\\n  \"personality\": \"an extremely happy person that likes to help people\",\\n  \"circumstances\": \"Your daughter will talk to you\"\\n}\\n[[ ## END FIRST PERSONA ## ]]\\n\\n[[ ## BEGIN SECOND PERSONA ## ]]\\n{\\n  \"name\": \"Alice\",\\n  \"role\": \"lovely daughter\",\\n  \"circumstances\": \"Your birthday is getting closer and you are talking with your dad to organize the party.You want your party to be themed as Lord of The Rings.\"\\n}\\n[[ ## END SECOND PERSONA ## ]]\\n\\n---\\n\\n\\n\\nInstructions:\\n  1. Always stay in character for both personas.\\n  2. The first utterance must be a short, generic greeting. Wait for a reply before continuing the conversation.\\n  3. Keep responses natural, concise, and appropriate for each persona.\\n  4. The conversation should have a clear beginning (greeting) and end (goodbye).', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': None, 'example': False}}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m[dialog_id] \u001b[35m1752069046672\u001b[0m\n",
      "\u001b[1m\u001b[95m[model] \u001b[35mmodel='qwen2.5:14b' temperature=0.8 seed=13 format={'$defs': {'Turn': {'description': 'Represents a single turn in a dialogue.\\n\\n:ivar speaker: The name or role of the speaker.\\n:vartype speaker: Optional[str]\\n:ivar text: The utterance text for this turn.\\n:vartype text: str', 'properties': {'speaker': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Speaker'}, 'text': {'title': 'Text', 'type': 'string'}}, 'required': ['speaker', 'text'], 'title': 'Turn', 'type': 'object'}}, 'description': 'Pydantic model for LLM-generated dialogue output.\\n\\n:ivar dialog: List of dialogue turns.\\n:vartype dialog: List[Turn]', 'properties': {'dialog': {'items': {'$ref': '#/$defs/Turn'}, 'title': 'Dialog', 'type': 'array'}}, 'required': ['dialog'], 'title': 'LLMDialogOutput', 'type': 'object'}\u001b[0m\n",
      "\u001b[1m\u001b[95m[seed] \u001b[35m4124850050\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Begins ---\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mHi Alice!\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mHello Dad! How are you today?\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mI'm great, thanks for asking. How about you? Are you looking forward to something special this week?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mYes, I am actually! Your birthday is coming up and I have a surprise planned!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mOh really? That sounds exciting. What kind of surprise do you have in mind?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mI was thinking of having your birthday party themed as Lord of The Rings! I thought it would be fun for everyone.\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mThat's such a great idea, Alice. It sounds like a lot of fun and you know how much I love that movie series. Do you need any help planning the event?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mYes please! You're always so good at organizing things. Could we start by figuring out what kind of decorations and food would fit with the theme?\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mAbsolutely, let's brainstorm together. For decorations, we could have a banner that says 'The Shire', some fake trees to represent the forest scenes, and maybe even a large statue of Gollum if we can find one or make one from papier-m\u00e2ch\u00e9.\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mThose sound like great ideas! What about food? I thought we could have some Hobbiton-style dishes with names that fit in the theme, like 'Hobbit Stew' and 'Shire Cake'.\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mThat sounds fantastic. We can also serve beverages such as 'Elven Wine' which would be red grape juice for those who don't drink alcohol. And of course, we'll need to have a special birthday cake designed like the One Ring or with little hobbit figurines on top.\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mI love it! This party is going to be amazing. Thanks for helping me Dad. I'm really excited about your birthday!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mYou're very welcome, dear. It's my pleasure and I can't wait to celebrate with you all in this wonderful theme.\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Ends ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sdialog.generators import PersonaDialogGenerator\n",
    "\n",
    "dialog_generator = PersonaDialogGenerator(\n",
    "    persona_a=bob_persona,\n",
    "    persona_b=alice_persona,\n",
    ")\n",
    "\n",
    "dialog_generator.generate().print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case: Dialogue Generation for STAR Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin this section, make sure you have the STAR dataset downloaded in your system, inside the `datasets` folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's clone the STAR dataset repository\n",
    "!git clone https://github.com/RasaHQ/STAR.git datasets/STAR\n",
    "\n",
    "# Let's check that `dialogues` and `tasks` folders are inside `datasets/STAR`\n",
    "!ls datasets/STAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [STAR](https://arxiv.org/pdf/2010.11853) dataset contains 6652 human-generated dialogues as JSON objects where files are named as `NUMBER.json`.\n",
    "\n",
    "Humans had to follow a well-defined set of instruction to generate the dialogue role playing the system (wizard) and the client (user).\n",
    "\n",
    "For instance, clicking [here](datasets/STAR/dialogues/1.json) we can open the file [`1.json`](datasets/STAR/dialogues/1.json) containing the first dialogue. For now, let's focus only on the `\"Scenario\"` field.\n",
    "\n",
    "For instance, for the dialogue in `1.json` it is as follows:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"Domains\": [  # List of domains\n",
    "        \"doctor\"\n",
    "    ],\n",
    "    \"Happy\": true,  # Wheather or not the dialogue follos a happy path\n",
    "    \"MultiTask\": false,  # Wheather or not this dialogue involves more than one task\n",
    "    \"UserTask\": \"You (Alexis) had an appointment with Dr. Morgan the other day. Unfortunately, you forgot to write down the instructions the doctor gave you. Please followup and find out how often to take your medicine.\",\n",
    "    \"WizardTask\": \"Inform the user of his/her doctor's orders.\",\n",
    "    \"WizardCapabilities\": [  # List of flowcharts describing the each task the Wizard is cable of doing\n",
    "        {\n",
    "        \"Domain\": \"doctor\",\n",
    "        \"SchemaImage\": \"doctor_followup.jpg\",\n",
    "        \"Task\": \"doctor_followup\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "We can use the `STAR` module from `sdialog` to read scenarios object from any dialogue in STAR given it's id given a STAR conversation id as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Domains': ['doctor'],\n",
       " 'Happy': True,\n",
       " 'MultiTask': False,\n",
       " 'UserTask': 'You (Alexis) had an appointment with Dr. Morgan the other day. Unfortunately, you forgot to write down the instructions the doctor gave you. Please followup and find out how often to take your medicine.',\n",
       " 'WizardCapabilities': [{'Domain': 'doctor',\n",
       "   'SchemaImage': 'doctor_followup.jpg',\n",
       "   'Task': 'doctor_followup'}],\n",
       " 'WizardTask': \"Inform the user of his/her doctor's orders.\"}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sdialog.datasets import STAR\n",
    "\n",
    "# Let's first indicate where the dataset is located\n",
    "STAR.set_path(\"datasets/STAR/\")\n",
    "\n",
    "# Let's set the first dialogue as the target example\n",
    "TARGET_DIALOG = 1\n",
    "\n",
    "# Let's load the scenario of the first dialog\n",
    "scenario = STAR.get_dialog_scenario(TARGET_DIALOG)\n",
    "scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which corresponds to the following dialogue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m[dialog_id] \u001b[35m1\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Begins ---\u001b[0m\n",
      "\u001b[94m[User] \u001b[0mHello, I'm really worried. I forgot what I'm supposed to do and forgot to write it down... What do I do?\u001b[0m\n",
      "\u001b[31m[System] \u001b[0mCould I get your name, please?\u001b[0m\n",
      "\u001b[94m[User] \u001b[0mMy name is Alexis and my last doctor was Dr. Morgan, but now my doctor is Dr. Johnson and I forgot how to take my medicine.\u001b[0m\n",
      "\u001b[31m[System] \u001b[0mYour instructions are: Take your medicine before you go to sleep. If you experience nausea, please contact your doctor immediately..\u001b[0m\n",
      "\u001b[94m[User] \u001b[0mAre you sure I'm supposed to take it before bed? I don't go to sleep every day because my sleep schedule is totally off right now because of the Coronavirus.\u001b[0m\n",
      "\u001b[31m[System] \u001b[0mYes. It must be before bed or it will not be effective.\u001b[0m\n",
      "\u001b[94m[User] \u001b[0mOkay thank you. I will get back in touch if this doesn't help.\u001b[0m\n",
      "\u001b[31m[System] \u001b[0mThank you and goodbye.\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Ends ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "original_dialogue = STAR.get_dialog(1)\n",
    "original_dialogue.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description-driven Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `scenario`, we can see that in this conversation, the user's behavior is defined by instructions given in natural language (`\"UserTask\"`), however, the system/wizard behavior is more rigidly defined as a graph describing the dialogue policy to followed (since system was expected to be more deterministic). These graphs are described as JSON objects storing the graph edges as key:value pairs (source:destination). We can find these graphs in the [`STAR/tasks`](datasets/STAR/tasks) folder.\n",
    "\n",
    "Ideally, we would like our `DialogGenerator` to generate dialogues for each different scenario. That is, given a `scenario` we would like generate multiple dialogues for it.\n",
    "\n",
    "To achieve this, we only need to find a way to describe each `scenario` using natural language so that we can pass it to our `DialogGenerator`.\n",
    "\n",
    "Fortunately, we can use the built-in `get_scenario_description()` method to do this, which takes an `scenario` as input and returns its natural language description containing all the details (including the system behavior described by the graphs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The conversation is between a User and a AI assistant in the following domains: doctor.\n",
      "\n",
      "The User instructions are: You (Alexis) had an appointment with Dr. Morgan the other day. Unfortunately, you forgot to write down the instructions the doctor gave you. Please followup and find out how often to take your medicine.\n",
      "The AI assistant instructions are: Inform the user of his/her doctor's orders.\n",
      "\n",
      "In addition, the AI assistant is instructed to follow specific flowcharts to address the tasks. Flowcharts are defined as graph described using DOT.\n",
      "The actual DOT for the current tasks are:\n",
      "\n",
      "The graph for the task 'doctor_followup' with domain 'doctor' is:\n",
      "```dot\n",
      "digraph doctor_followup  {\n",
      "    hello -> ask_name;\n",
      "    ask_name -> doctor_ask_doctor_name;\n",
      "    doctor_ask_doctor_name -> query;\n",
      "    query -> doctor_inform_doctors_instructions;\n",
      "    doctor_inform_doctors_instructions -> anything_else\n",
      "}\n",
      "```\n",
      "and one example responses for each node is provided in the following json:\n",
      "```json\n",
      "{\n",
      "  \"hello\": \"Hello, how can I help?\",\n",
      "  \"ask_name\": \"Could I get your name, please?\",\n",
      "  \"doctor_ask_doctor_name\": \"Who is your doctor?\",\n",
      "  \"doctor_inform_doctors_instructions\": \"Your instructions are: INSTRUCTIONS.\",\n",
      "  \"doctor_bye\": \"Thank you and goodbye.\",\n",
      "  \"anything_else\": \"Is there anything else that I can do for you?\"\n",
      "}\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Finally, the following should be considered regarding the conversation:\n",
      "   1. The conversation follows the 'happy path', meaning the conversations goes according to what it is described in the flowcharts.\n",
      "   2. The user is calling to perform only the defined task (doctor_followup), nothing else.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(STAR.get_scenario_description(scenario))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the original graph in JSON describing the system's behavior have been converted to a [DOT](https://en.wikipedia.org/wiki/DOT_(graph_description_language)) description which should be easier to interpret by the LLM, since DOT is a well-known format to describe graphs in plain text. \n",
    "\n",
    "Let's now put these two methods together and create a function that given a STAR dialogue ID will generate a natural language description of the scenario associated to it, simply as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dialog_scenario_description(dialogue_id):\n",
    "    # Get the scenario of the target dialogue\n",
    "    scenario = STAR.get_dialog_scenario(dialogue_id)\n",
    "    # Then return it along its description in natural language\n",
    "    return scenario, STAR.get_scenario_description(scenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this function now we have everything we need to generate synthethic dialogues for STAR that follows the same scenario as a given target real dialogue.\n",
    "\n",
    "For instance, let's say we want to generate dialogues following the same scenario as the first STAR dialogue, we can simply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's get the scenario and description of the first dialogue\n",
    "scenario, description = get_dialog_scenario_description(dialogue_id=1)\n",
    "\n",
    "# le'ts now create a dialogue generator for it\n",
    "dialog_generator = DialogGenerator(\n",
    "    dialogue_details=description,\n",
    "    scenario=scenario\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now generate multiple conversation that follows the same scenario as dialogue 1 of STAR dataset.\n",
    "> **Note**\n",
    "> Run the cell multiple times to get different conversations for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-09 15:51:33] INFO:sdialog.generators:Prompt used: [{'type': 'system', 'data': {'content': 'You are a highly capable AI assistant skilled at writing natural, multi-turn conversations by role-playing different speakers. Generate a complete, realistic dialogue from start (greetings) to finish (goodbye), ensuring each speaker remains consistent and the flow is natural and engaging.\\n\\n', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'system', 'name': None, 'id': None}}, {'type': 'human', 'data': {'content': 'The conversation is between a User and a AI assistant in the following domains: doctor.\\n\\nThe User instructions are: You (Alexis) had an appointment with Dr. Morgan the other day. Unfortunately, you forgot to write down the instructions the doctor gave you. Please followup and find out how often to take your medicine.\\nThe AI assistant instructions are: Inform the user of his/her doctor\\'s orders.\\n\\nIn addition, the AI assistant is instructed to follow specific flowcharts to address the tasks. Flowcharts are defined as graph described using DOT.\\nThe actual DOT for the current tasks are:\\n\\nThe graph for the task \\'doctor_followup\\' with domain \\'doctor\\' is:\\n```dot\\ndigraph doctor_followup  {\\n    hello -> ask_name;\\n    ask_name -> doctor_ask_doctor_name;\\n    doctor_ask_doctor_name -> query;\\n    query -> doctor_inform_doctors_instructions;\\n    doctor_inform_doctors_instructions -> anything_else\\n}\\n```\\nand one example responses for each node is provided in the following json:\\n```json\\n{\\n  \"hello\": \"Hello, how can I help?\",\\n  \"ask_name\": \"Could I get your name, please?\",\\n  \"doctor_ask_doctor_name\": \"Who is your doctor?\",\\n  \"doctor_inform_doctors_instructions\": \"Your instructions are: INSTRUCTIONS.\",\\n  \"doctor_bye\": \"Thank you and goodbye.\",\\n  \"anything_else\": \"Is there anything else that I can do for you?\"\\n}\\n```\\n\\n---\\n\\n\\nFinally, the following should be considered regarding the conversation:\\n   1. The conversation follows the \\'happy path\\', meaning the conversations goes according to what it is described in the flowcharts.\\n   2. The user is calling to perform only the defined task (doctor_followup), nothing else.\\n', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': None, 'example': False}}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m[dialog_id] \u001b[35m1752069093284\u001b[0m\n",
      "\u001b[1m\u001b[95m[model] \u001b[35mmodel='qwen2.5:14b' temperature=0.8 seed=13 format={'$defs': {'Turn': {'description': 'Represents a single turn in a dialogue.\\n\\n:ivar speaker: The name or role of the speaker.\\n:vartype speaker: Optional[str]\\n:ivar text: The utterance text for this turn.\\n:vartype text: str', 'properties': {'speaker': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Speaker'}, 'text': {'title': 'Text', 'type': 'string'}}, 'required': ['speaker', 'text'], 'title': 'Turn', 'type': 'object'}}, 'description': 'Pydantic model for LLM-generated dialogue output.\\n\\n:ivar dialog: List of dialogue turns.\\n:vartype dialog: List[Turn]', 'properties': {'dialog': {'items': {'$ref': '#/$defs/Turn'}, 'title': 'Dialog', 'type': 'array'}}, 'required': ['dialog'], 'title': 'LLMDialogOutput', 'type': 'object'}\u001b[0m\n",
      "\u001b[1m\u001b[95m[seed] \u001b[35m2449260168\u001b[0m\n",
      "\u001b[1m\u001b[95m[scenario] \u001b[35m\u001b[0m\n",
      "\u001b[35m{\n",
      "  \"Domains\": [\n",
      "    \"doctor\"\n",
      "  ],\n",
      "  \"Happy\": true,\n",
      "  \"MultiTask\": false,\n",
      "  \"UserTask\": \"You (Alexis) had an appointment with Dr. Morgan the other day. Unfortunately, you forgot to write down the instructions the doctor gave you. Please followup and find out how often to take your medicine.\",\n",
      "  \"WizardCapabilities\": [\n",
      "    {\n",
      "      \"Domain\": \"doctor\",\n",
      "      \"SchemaImage\": \"doctor_followup.jpg\",\n",
      "      \"Task\": \"doctor_followup\"\n",
      "    }\n",
      "  ],\n",
      "  \"WizardTask\": \"Inform the user of his/her doctor's orders.\"\n",
      "}\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Begins ---\u001b[0m\n",
      "\u001b[31m[AI Assistant] \u001b[0mHello, how can I help?\u001b[0m\n",
      "\u001b[94m[User] \u001b[0mCould you please help me find out the instructions for my medicine from Dr. Morgan?\u001b[0m\n",
      "\u001b[31m[AI Assistant] \u001b[0mCould I get your name, please?\u001b[0m\n",
      "\u001b[94m[User] \u001b[0mMy name is Alexis.\u001b[0m\n",
      "\u001b[31m[AI Assistant] \u001b[0mWho is your doctor?\u001b[0m\n",
      "\u001b[94m[User] \u001b[0mIt's Dr. Morgan.\u001b[0m\n",
      "\u001b[31m[AI Assistant] \u001b[0mYour instructions are: Take the medicine three times a day, 30 minutes before meals.\u001b[0m\n",
      "\u001b[94m[User] \u001b[0mThank you so much!\u001b[0m\n",
      "\u001b[31m[AI Assistant] \u001b[0mIs there anything else that I can do for you?\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Ends ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dialog = dialog_generator.generate()\n",
    "dialog.print(scenario=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the LLM is able to follow the scenario surprisingly well, specially for the system part which is guided by a graph with pre-defined responses.\n",
    "\n",
    "Now update the generator to match a more challenging scenario, let's say that of dialogue 5100 that is multi-task and does not follow a happy path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Domains': ['plane', 'weather'],\n",
       " 'Happy': False,\n",
       " 'MultiTask': True,\n",
       " 'UserTask': 'Come up with your own scenario!\\n\\nAbout you:\\n- Your name: Ben\\n\\n The AI Assistant can handle:\\n- Search for a flight (e.g. from Chicago to Pittsburgh)\\n- Book a flight (e.g. with id 193)\\n- Checking the weather forecast in different Cities (e.g. Chicago or Pittsburgh)',\n",
       " 'WizardCapabilities': [{'Domain': 'plane',\n",
       "   'SchemaImage': 'plane_search.jpg',\n",
       "   'Task': 'plane_search'},\n",
       "  {'Domain': 'plane', 'SchemaImage': 'plane_book.jpg', 'Task': 'plane_book'},\n",
       "  {'Domain': 'weather', 'SchemaImage': 'weather.jpg', 'Task': 'weather'}],\n",
       " 'WizardTask': 'Follow the flow charts and help the user.'}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scenario, description = get_dialog_scenario_description(5100)\n",
    "\n",
    "dialog_generator = DialogGenerator(\n",
    "    dialogue_details=description,\n",
    "    scenario=scenario\n",
    ")\n",
    "scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And generate dialogues for it:\n",
    "\n",
    "_(run multi-times the call to generate different ones)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-09 15:54:04] INFO:sdialog.generators:Prompt used: [{'type': 'system', 'data': {'content': 'You are a highly capable AI assistant skilled at writing natural, multi-turn conversations by role-playing different speakers. Generate a complete, realistic dialogue from start (greetings) to finish (goodbye), ensuring each speaker remains consistent and the flow is natural and engaging.\\n\\n', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'system', 'name': None, 'id': None}}, {'type': 'human', 'data': {'content': 'The conversation is between a User and a AI assistant in the following domains: plane, weather.\\n\\nThe User instructions are: Come up with your own scenario!\\n\\nAbout you:\\n- Your name: Ben\\n\\n The AI Assistant can handle:\\n- Search for a flight (e.g. from Chicago to Pittsburgh)\\n- Book a flight (e.g. with id 193)\\n- Checking the weather forecast in different Cities (e.g. Chicago or Pittsburgh)\\nThe AI assistant instructions are: Follow the flow charts and help the user.\\n\\nIn addition, the AI assistant is instructed to follow specific flowcharts to address the tasks. Flowcharts are defined as graph described using DOT.\\nThe actual DOT for the current tasks are:\\n\\nThe graph for the task \\'plane_search\\' with domain \\'plane\\' is:\\n```dot\\ndigraph plane_search  {\\n    hello -> ask_name;\\n    ask_name -> plane_ask_departure_city;\\n    plane_ask_departure_city -> plane_ask_arrival_city;\\n    plane_ask_arrival_city -> plane_ask_date;\\n    plane_ask_date -> plane_request_optional;\\n    plane_request_optional -> query;\\n    query -> plane_inform_flight_details;\\n    plane_inform_flight_details -> plane_ask_more_questions;\\n    yes -> plane_request_optional;\\n    no -> goodbye_1\\n}\\n```\\nand one example responses for each node is provided in the following json:\\n```json\\n{\\n  \"hello\": \"Hello, how can I help?\",\\n  \"ask_name\": \"May I have your name, please?\",\\n  \"plane_ask_departure_city\": \"Where are you departing from?\",\\n  \"plane_ask_arrival_city\": \"Where are you going to?\",\\n  \"plane_ask_date\": \"On what day do you want the plane to arrive?\",\\n  \"plane_request_optional\": \"I can also filter the results for a specific airline, class, price or duration.\",\\n  \"plane_inform_flight_details\": \"Right, I found CLAZZ flight with AIRLINE for PRICE credits\\\\n that takes DURATION hours to get to ARRIVAL_CITY.\",\\n  \"plane_ask_more_questions\": \"Would you like to search for any more flights?\",\\n  \"goodbye_1\": \"Thank you and goodbye!\",\\n  \"anything_else\": \"Is there anything else that I can do for you?\"\\n}\\n```\\n\\n---\\n\\nThe graph for the task \\'plane_book\\' with domain \\'plane\\' is:\\n```dot\\ndigraph plane_book  {\\n    hello -> ask_name;\\n    ask_name -> plane_ask_flight_id;\\n    plane_ask_flight_id -> query_check;\\n    available -> plane_flight_available;\\n    unavailable -> plane_flight_unavailable;\\n    yes -> query_book;\\n    no -> plane_ask_flight_id;\\n    query_success -> plane_reservation_succeeded;\\n    query_failure -> plane_reservation_failed;\\n    plane_reservation_succeeded -> anything_else;\\n    plane_reservation_failed -> anything_else\\n}\\n```\\nand one example responses for each node is provided in the following json:\\n```json\\n{\\n  \"hello\": \"Hello, how can I help?\",\\n  \"ask_name\": \"May I have your name, please?\",\\n  \"plane_ask_flight_id\": \"Can I have your flight ID, please?\",\\n  \"plane_flight_available\": \"The flight is available. Should I reserve it for you?\",\\n  \"plane_flight_unavailable\": \"Sorry, but the flight with id \\'FLIGHT_ID\\' is not available any more.\",\\n  \"plane_reservation_succeeded\": \"Right, your flight is now reserved!\",\\n  \"plane_reservation_failed\": \"I\\'m sorry, but your reservation request was unsuccessful.\",\\n  \"goodbye_1\": \"Thank you and goodbye.\",\\n  \"anything_else\": \"Is there anything else that I can do for you?\"\\n}\\n```\\n\\n---\\n\\nThe graph for the task \\'weather\\' with domain \\'weather\\' is:\\n```dot\\ndigraph weather  {\\n    hello -> weather_ask_day;\\n    weather_ask_day -> weather_ask_location;\\n    weather_ask_location -> query;\\n    query -> weather_inform_forecast;\\n    weather_inform_forecast -> anything_else\\n}\\n```\\nand one example responses for each node is provided in the following json:\\n```json\\n{\\n  \"hello\": \"Hello, how can I help?\",\\n  \"weather_ask_day\": \"For what day would you like the weather forecast?\",\\n  \"weather_ask_location\": \"For what location would you like the weather forecast?\",\\n  \"weather_inform_forecast\": \"It will be WEATHER all day on DAY in CITY, with temperatures of around TEMPERATURE degrees celsius.\",\\n  \"weather_bye\": \"Thank you and goodbye.\",\\n  \"anything_else\": \"Is there anything else that I can do for you?\"\\n}\\n```\\n\\n---\\n\\n\\nFinally, the following should be considered regarding the conversation:\\n   1. The conversation does NOT follow a \\'happy path\\', meaning something happend to the user to change its mind or something happend in the environment for the conversation to not flow as expected, as described in the flowchart.\\n   2. The user is calling to perform multiple tasks, involving all the tasks defined as flowcharts above (plane_search, plane_book, weather).\\n', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': None, 'example': False}}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m[dialog_id] \u001b[35m1752069244977\u001b[0m\n",
      "\u001b[1m\u001b[95m[model] \u001b[35mmodel='qwen2.5:14b' temperature=0.8 seed=13 format={'$defs': {'Turn': {'description': 'Represents a single turn in a dialogue.\\n\\n:ivar speaker: The name or role of the speaker.\\n:vartype speaker: Optional[str]\\n:ivar text: The utterance text for this turn.\\n:vartype text: str', 'properties': {'speaker': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Speaker'}, 'text': {'title': 'Text', 'type': 'string'}}, 'required': ['speaker', 'text'], 'title': 'Turn', 'type': 'object'}}, 'description': 'Pydantic model for LLM-generated dialogue output.\\n\\n:ivar dialog: List of dialogue turns.\\n:vartype dialog: List[Turn]', 'properties': {'dialog': {'items': {'$ref': '#/$defs/Turn'}, 'title': 'Dialog', 'type': 'array'}}, 'required': ['dialog'], 'title': 'LLMDialogOutput', 'type': 'object'}\u001b[0m\n",
      "\u001b[1m\u001b[95m[seed] \u001b[35m33572775\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Begins ---\u001b[0m\n",
      "\u001b[31m[AI Assistant] \u001b[0mHello, how can I help?\u001b[0m\n",
      "\u001b[94m[Ben] \u001b[0mHi there. Can you find a flight for me from Chicago to Pittsburgh and also check the weather in both places?\u001b[0m\n",
      "\u001b[31m[AI Assistant] \u001b[0mSure thing! May I have your name, please?\u001b[0m\n",
      "\u001b[94m[Ben] \u001b[0mMy name is Ben.\u001b[0m\n",
      "\u001b[31m[AI Assistant] \u001b[0mWhere are you departing from?\u001b[0m\n",
      "\u001b[94m[Ben] \u001b[0mI am leaving from Chicago.\u001b[0m\n",
      "\u001b[31m[AI Assistant] \u001b[0mWhere are you going to?\u001b[0m\n",
      "\u001b[94m[Ben] \u001b[0mHeading to Pittsburgh.\u001b[0m\n",
      "\u001b[31m[AI Assistant] \u001b[0mOn what day do you want the plane to arrive?\u001b[0m\n",
      "\u001b[94m[Ben] \u001b[0mI need a flight on Thursday, please.\u001b[0m\n",
      "\u001b[31m[AI Assistant] \u001b[0mI can also filter the results for a specific airline, class, price or duration. Do you have any preferences in mind?\u001b[0m\n",
      "\u001b[94m[Ben] \u001b[0mNo particular preference, just find me something affordable.\u001b[0m\n",
      "\u001b[31m[AI Assistant] \u001b[0mRight, I found CLAZZ flight with AIRLINE for 250 credits that takes 1 hour to get to Pittsburgh. Would you like to search for any more flights?\u001b[0m\n",
      "\u001b[94m[Ben] \u001b[0mThat sounds good, let's book it then.\u001b[0m\n",
      "\u001b[31m[AI Assistant] \u001b[0mGreat! Can I have your flight ID, please? It was 193.\u001b[0m\n",
      "\u001b[94m[Ben] \u001b[0mThe flight id is 193.\u001b[0m\n",
      "\u001b[31m[AI Assistant] \u001b[0mThe flight is available. Should I reserve it for you?\u001b[0m\n",
      "\u001b[94m[Ben] \u001b[0mYes, please book it now.\u001b[0m\n",
      "\u001b[31m[AI Assistant] \u001b[0mRight, your flight is now reserved! Is there anything else that I can do for you?\u001b[0m\n",
      "\u001b[94m[Ben] \u001b[0mActually, could you also check the weather forecast in Chicago and Pittsburgh on Thursday?\u001b[0m\n",
      "\u001b[31m[AI Assistant] \u001b[0mOf course! For what day would you like the weather forecast?\u001b[0m\n",
      "\u001b[94m[Ben] \u001b[0mI need it for Thursday.\u001b[0m\n",
      "\u001b[31m[AI Assistant] \u001b[0mFor what location would you like the weather forecast?\u001b[0m\n",
      "\u001b[94m[Ben] \u001b[0mFirst Chicago, then Pittsburgh.\u001b[0m\n",
      "\u001b[31m[AI Assistant] \u001b[0mIt will be cloudy all day on Thursday in Chicago, with temperatures of around 18 degrees celsius. For Pittsburgh it will be partly sunny with a high of about 22 degrees celsius.\u001b[0m\n",
      "\u001b[94m[Ben] \u001b[0mThanks for the weather update too!\u001b[0m\n",
      "\u001b[31m[AI Assistant] \u001b[0mYou're welcome, Ben! Is there anything else that I can do for you?\u001b[0m\n",
      "\u001b[94m[Ben] \u001b[0mNo, that's all. Thanks a lot.\u001b[0m\n",
      "\u001b[31m[AI Assistant] \u001b[0mThank you and goodbye!\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Ends ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dialog_generator.generate().print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Role-playing-based Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before, in previous section we only had to find a way to describe each `scenario` using natural language so that we can pass it to our `DialogGenerator`.\n",
    "\n",
    "Likewise, now we have to find a way to create the right system and user personas for each scenario which means we have to return the right system and user `Persona`s for a given `scenario`.\n",
    "\n",
    "Fortunately, we can use the built-in `STAR.get_user_persona_for_scenario(scenario)` and `STAR.get_system_persona_for_scenario(scenario)` methods to achieve this.\n",
    "\n",
    "For instance, let's get the user persona for the `scenario` of the first dialogue above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Language: English\n",
      "* Role: user calling a AI assistant that can perform multiple tasks in the following domains: doctor.\n",
      "\n",
      "The following should be considered regarding the conversation:\n",
      "   1. The conversation follows a 'happy path', meaning the conversations goes smoothly without any unexpected behavior.\n",
      "   2. The conversation involves only one task you were instructed to (doctor_followup), nothing else\n",
      "* Circumstances: You (Alexis) had an appointment with Dr. Morgan the other day. Unfortunately, you forgot to write down the instructions the doctor gave you. Please followup and find out how often to take your medicine.\n"
     ]
    }
   ],
   "source": [
    "scenario = STAR.get_dialog_scenario(TARGET_DIALOG)\n",
    "\n",
    "user_persona = STAR.get_user_persona_for_scenario(scenario)\n",
    "print(user_persona)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, similar to what we did in the previous subsection, we just need to define a function that given a dialogue ID can return its scenario as well as the system and user persona for it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dialog_scenario_and_personas(dialogue_id):\n",
    "    # Get the scenario of the target dialogue\n",
    "    scenario = STAR.get_dialog_scenario(dialogue_id)\n",
    "    # Get the personas\n",
    "    system = STAR.get_system_persona_for_scenario(scenario)\n",
    "    user = STAR.get_user_persona_for_scenario(scenario)\n",
    "    return scenario, system, user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it, not we can simply create a `PersonaDialogGenerator` using the system and user personas as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-09 15:55:10] INFO:sdialog.generators:Prompt used: [{'type': 'system', 'data': {'content': 'You are a highly capable AI assistant skilled at writing natural, multi-turn conversations by role-playing different speakers. Generate a complete, realistic dialogue from start (greetings) to finish (goodbye), ensuring each speaker remains consistent and the flow is natural and engaging.\\n\\n', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'system', 'name': None, 'id': None}}, {'type': 'human', 'data': {'content': '# Improved two-persona dialogue prompt\\nRole-play a conversation between the following two characters. Each character is defined by their persona below in JSON format. Remain in character for both roles at all times.\\n\\n[[ ## BEGIN FIRST PERSONA ## ]]\\n{\\n  \"language\": \"English\",\\n  \"role\": \"AI assistant.\\\\nIn the conversation, the AI assistant is instructed to follow specific action flowcharts to address the tasks. Flowcharts are defined as graph described using DOT.\\\\nThe actual DOT for the current tasks are:\\\\n\\\\n## doctor_followup (doctor)\\\\n\\\\nThe flowchart described as an action transition graph for the task \\'doctor_followup\\' with domain \\'doctor\\' is:\\\\n```dot\\\\ndigraph doctor_followup  {\\\\n    hello -> ask_name;\\\\n    ask_name -> doctor_ask_doctor_name;\\\\n    doctor_ask_doctor_name -> query;\\\\n    query -> doctor_inform_doctors_instructions;\\\\n    doctor_inform_doctors_instructions -> anything_else\\\\n}\\\\n```\\\\nResponse example for each action is provided in the following json:\\\\n```json\\\\n{\\\\n  \\\\\"hello\\\\\": \\\\\"Hello, how can I help?\\\\\",\\\\n  \\\\\"ask_name\\\\\": \\\\\"Could I get your name, please?\\\\\",\\\\n  \\\\\"doctor_ask_doctor_name\\\\\": \\\\\"Who is your doctor?\\\\\",\\\\n  \\\\\"doctor_inform_doctors_instructions\\\\\": \\\\\"Your instructions are: INSTRUCTIONS.\\\\\",\\\\n  \\\\\"doctor_bye\\\\\": \\\\\"Thank you and goodbye.\\\\\",\\\\n  \\\\\"anything_else\\\\\": \\\\\"Is there anything else that I can do for you?\\\\\"\\\\n}\\\\n```\\\\nwhere UPPERCASE words above are just example placeholders. You MUST fill in those with any coherent values in the actual conversation.\\\\n\\\\n\",\\n  \"circumstances\": \"Inform the user of his/her doctor\\'s orders.\"\\n}\\n[[ ## END FIRST PERSONA ## ]]\\n\\n[[ ## BEGIN SECOND PERSONA ## ]]\\n{\\n  \"language\": \"English\",\\n  \"role\": \"user calling a AI assistant that can perform multiple tasks in the following domains: doctor.\\\\n\\\\nThe following should be considered regarding the conversation:\\\\n   1. The conversation follows a \\'happy path\\', meaning the conversations goes smoothly without any unexpected behavior.\\\\n   2. The conversation involves only one task you were instructed to (doctor_followup), nothing else\",\\n  \"circumstances\": \"You (Alexis) had an appointment with Dr. Morgan the other day. Unfortunately, you forgot to write down the instructions the doctor gave you. Please followup and find out how often to take your medicine.\"\\n}\\n[[ ## END SECOND PERSONA ## ]]\\n\\n---\\n\\n\\n\\nInstructions:\\n  1. Always stay in character for both personas.\\n  2. The first utterance must be a short, generic greeting. Wait for a reply before continuing the conversation.\\n  3. Keep responses natural, concise, and appropriate for each persona.\\n  4. The conversation should have a clear beginning (greeting) and end (goodbye).', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': None, 'example': False}}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m[dialog_id] \u001b[35m1752069310746\u001b[0m\n",
      "\u001b[1m\u001b[95m[model] \u001b[35mmodel='qwen2.5:14b' temperature=0.8 seed=13 format={'$defs': {'Turn': {'description': 'Represents a single turn in a dialogue.\\n\\n:ivar speaker: The name or role of the speaker.\\n:vartype speaker: Optional[str]\\n:ivar text: The utterance text for this turn.\\n:vartype text: str', 'properties': {'speaker': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Speaker'}, 'text': {'title': 'Text', 'type': 'string'}}, 'required': ['speaker', 'text'], 'title': 'Turn', 'type': 'object'}}, 'description': 'Pydantic model for LLM-generated dialogue output.\\n\\n:ivar dialog: List of dialogue turns.\\n:vartype dialog: List[Turn]', 'properties': {'dialog': {'items': {'$ref': '#/$defs/Turn'}, 'title': 'Dialog', 'type': 'array'}}, 'required': ['dialog'], 'title': 'LLMDialogOutput', 'type': 'object'}\u001b[0m\n",
      "\u001b[1m\u001b[95m[seed] \u001b[35m436821777\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Begins ---\u001b[0m\n",
      "\u001b[31m[AI assistant] \u001b[0mHello, how can I help?\u001b[0m\n",
      "\u001b[94m[user] \u001b[0mHi, could you remind me of Dr. Morgan's instructions after my last appointment? I forgot to write them down.\u001b[0m\n",
      "\u001b[31m[AI assistant] \u001b[0mCould I get your name, please?\u001b[0m\n",
      "\u001b[94m[user] \u001b[0mSure, it\u2019s Alexis.\u001b[0m\n",
      "\u001b[31m[AI assistant] \u001b[0mWho is your doctor?\u001b[0m\n",
      "\u001b[94m[user] \u001b[0mIt's Dr. Morgan from the city hospital.\u001b[0m\n",
      "\u001b[31m[AI assistant] \u001b[0mYour instructions are: Take your medication three times a day, 30 minutes before meals. Ensure you have regular follow-up appointments every six months.\u001b[0m\n",
      "\u001b[94m[user] \u001b[0mThank you! That's exactly what I needed to remember.\u001b[0m\n",
      "\u001b[31m[AI assistant] \u001b[0mIs there anything else that I can do for you?\u001b[0m\n",
      "\u001b[94m[user] \u001b[0mNo, that\u2019s all. Thanks so much!\u001b[0m\n",
      "\u001b[31m[AI assistant] \u001b[0mThank you and goodbye.\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Ends ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# let's get the personas and the scenario\n",
    "scenario, system, user = get_dialog_scenario_and_personas(dialogue_id=1)\n",
    "\n",
    "# le'ts now create a dialogue generator for it\n",
    "dialog_generator = PersonaDialogGenerator(\n",
    "    persona_a=system,\n",
    "    persona_b=user,\n",
    "    scenario=scenario\n",
    ")\n",
    "\n",
    "# let's generate the dialogue\n",
    "dialog_generator.generate().print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it for this tutorial, congrats! \ud83d\ude0e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving our dialogues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's generate one synthetic dialog for each happy `\"doctor_followup\"` dialog in STAR and save it to disk for later use.\n",
    "\n",
    "Let's first get all happy dialogues for this task using `sdialog`'s built-in `STAR.get_dialogs()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of happy \"doctor_followup\" dialogues in STAR: 105\n"
     ]
    }
   ],
   "source": [
    "original_dialogs = STAR.get_dialogs(task_name=\"doctor_followup\", happy=True, multitask=False)\n",
    "print('Total number of happy \"doctor_followup\" dialogues in STAR:', len(original_dialogs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate the dialogues and save them in the path pointed by the `PATH_OUTPUT` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "PATH_OUTPUT = \"output/STAR/full-generation\"\n",
    "\n",
    "path_txt = os.path.join(PATH_OUTPUT, \"txt\")\n",
    "path_json = os.path.join(PATH_OUTPUT, \"json\")\n",
    "os.makedirs(path_txt, exist_ok=True)\n",
    "os.makedirs(path_json, exist_ok=True)\n",
    "\n",
    "for dialog in tqdm(original_dialogs, desc=\"Dialog generation\"):\n",
    "    if os.path.exists(os.path.join(path_txt, f\"{dialog.id}.txt\")):\n",
    "        continue\n",
    "\n",
    "    scenario, description = STAR.get_dialog_scenario_description(dialog.id)\n",
    "    dialog_generator = DialogGenerator(\n",
    "        dialogue_details=description,\n",
    "        scenario=scenario\n",
    "    )\n",
    "    dialog = dialog_generator.generate(id=dialog.id, seed=dialog.id)\n",
    "\n",
    "    # Normalize speaker names in each turn (since their also generated by the LLM)\n",
    "    for turn in dialog.turns:\n",
    "        turn.speaker = \"System\" if \"AI\" in turn.speaker else \"User\"\n",
    "\n",
    "    dialog.to_file(os.path.join(path_json, f\"{dialog.id}.json\"))\n",
    "    dialog.to_file(os.path.join(path_txt, f\"{dialog.id}.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's check the files were generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mjson\u001b[0m/\n",
      "\u001b[01;34mtxt\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "%ls output/STAR/full-generation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.txt\n",
      "1848.txt\n",
      "1886.txt\n",
      "1896.txt\n",
      "1899.txt\n",
      "1942.txt\n",
      "1963.txt\n",
      "1970.txt\n",
      "2103.txt\n",
      "2228.txt\n",
      "2273.txt\n",
      "2487.txt\n",
      "2526.txt\n",
      "2578.txt\n",
      "2579.txt\n",
      "2624.txt\n",
      "2699.txt\n",
      "2733.txt\n",
      "3007.txt\n",
      "3024.txt\n",
      "3050.txt\n",
      "3071.txt\n",
      "3073.txt\n",
      "3086.txt\n",
      "3088.txt\n",
      "3110.txt\n",
      "3116.txt\n",
      "3126.txt\n",
      "3136.txt\n",
      "3155.txt\n",
      "3198.txt\n",
      "3202.txt\n",
      "3210.txt\n",
      "3234.txt\n",
      "3254.txt\n",
      "3264.txt\n",
      "3269.txt\n",
      "3274.txt\n",
      "3298.txt\n",
      "3316.txt\n",
      "3323.txt\n",
      "3329.txt\n",
      "3330.txt\n",
      "3356.txt\n",
      "3371.txt\n",
      "3391.txt\n",
      "3403.txt\n",
      "3418.txt\n",
      "3422.txt\n",
      "3437.txt\n",
      "3446.txt\n",
      "3454.txt\n",
      "3469.txt\n",
      "3494.txt\n",
      "3516.txt\n",
      "3528.txt\n",
      "3550.txt\n",
      "3652.txt\n",
      "3675.txt\n",
      "3743.txt\n",
      "3769.txt\n",
      "4055.txt\n",
      "4058.txt\n",
      "4067.txt\n",
      "4076.txt\n",
      "4082.txt\n",
      "4093.txt\n",
      "4100.txt\n",
      "4111.txt\n",
      "4159.txt\n",
      "4173.txt\n",
      "4191.txt\n",
      "4205.txt\n",
      "4214.txt\n",
      "4224.txt\n",
      "4231.txt\n",
      "4233.txt\n",
      "4253.txt\n",
      "4255.txt\n",
      "4263.txt\n",
      "4341.txt\n",
      "4349.txt\n",
      "4358.txt\n",
      "4381.txt\n",
      "4395.txt\n",
      "4403.txt\n",
      "4416.txt\n",
      "4459.txt\n",
      "4468.txt\n",
      "4477.txt\n",
      "4502.txt\n",
      "4515.txt\n",
      "4524.txt\n",
      "4536.txt\n",
      "4570.txt\n",
      "4591.txt\n",
      "4623.txt\n",
      "4653.txt\n",
      "4737.txt\n",
      "4743.txt\n",
      "4752.txt\n",
      "4851.txt\n",
      "4870.txt\n",
      "4875.txt\n",
      "9.txt\n"
     ]
    }
   ],
   "source": [
    "%ls output/STAR/full-generation/txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Doctor-Patient Conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose now you have to generate synthetic doctor-patient conversations, is it better to do it using the \"description\" or the \"role-playing\" approach?\n",
    "\n",
    "- How would you define the Patient and Doctor personas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: do your magic!\n",
    "class DoctorPersona(BasePersona):\n",
    "    pass\n",
    "\n",
    "class PatientPersona(BasePersona):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How would you initialize them? (e.g. concrete values for each defined attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doctor = DoctorPersona(\n",
    "    # TODO: assign values to the attribues!\n",
    ")\n",
    "patient = PatientPersona(\n",
    "    # TODO: assign values to the attribues!\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have defined our Personas and created our two doctor and patient personas, wwe can simply create a generator for them as we did before in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# le'ts now create a dialogue generator for our doctor and patients\n",
    "dialog_generator = PersonaDialogGenerator(\n",
    "    persona_a=doctor,\n",
    "    persona_b=patient\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And generate as many dialogues for them: _(running the cell multiple times)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's generate the doctor-patient dialogue\n",
    "dialog_generator.generate().print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Can you think of an `scenario` object for doctor-patient conversations? What would this scenario contain?\n",
    "\n",
    "> \ud83d\udca1 **Hint:** what is it that you want to keep track/control when generating the conversations? (different dissises? different outcomes? different skills? etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = {}  # TODO: define your own, perhaps better to use pydantic's BaseModel instead of a dict {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we had such `scenario` then we could define a function that could return the right doctor and patient for the provided scenario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doctor_patient_for_scenario(scenario):\n",
    "    # TODO: do some magic to return the right doctor\n",
    "    #       and patient personas for the given scenario\n",
    "    # doctor = DoctorPersona()\n",
    "    # patient = DoctorPersona()\n",
    "    return doctor, patient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which we could finally use to create generators for different `scenarios`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generator_for_scenario(scenario):\n",
    "    # Get the right doctor and patient personas for the given scenario\n",
    "    doctor, patient = get_doctor_patient_for_scenario(scenario)\n",
    "\n",
    "    # Create and return a dialogue generator for them\n",
    "    return PersonaDialogGenerator(\n",
    "        persona_a=doctor,\n",
    "        persona_b=patient,\n",
    "        scenario=scenario,\n",
    "        # dialogue_details=\"\"  # TODO: optional, in case scenario also requires defining certain properties outside the personas\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = get_generator_for_scenario(scenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which will allow us to generate multiple dialogues belonging to the same `scenario`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, huh? Congrats for finalizing the tutorial! you did a great job! \ud83d\ude0e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}