{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Dialogue Generation\n",
    "\n",
    "<p align=\"right\" style=\"margin-right: 8px;\">\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/idiap/sdialog/blob/main/tutorials/00_overview/1.single_llm_full_generation.ipynb\">\n",
    "        <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first check if our environment is all set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in Jupyter Notebook\n"
     ]
    }
   ],
   "source": [
    "# Setup the environment depending on weather we are running in Google Colab or Jupyter Notebook\n",
    "from IPython import get_ipython\n",
    "\n",
    "if \"google.colab\" in str(get_ipython()):\n",
    "    print(\"Running on CoLab\")\n",
    "    # Downloading only the \"output\" directory from the repository\n",
    "    !git init .\n",
    "    !git remote add -f origin https://github.com/Play-Your-Part/tutorials.git\n",
    "    !git config core.sparseCheckout true\n",
    "    !echo \"output\" >> .git/info/sparse-checkout\n",
    "    !git pull origin main\n",
    "\n",
    "    # Installing Ollama\n",
    "    !curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "    # Installing sdialog\n",
    "    !git clone https://github.com/idiap/sdialog.git\n",
    "    %cd sdialog\n",
    "    %pip install -e .\n",
    "    %cd ..\n",
    "else:\n",
    "    print(\"Running in Jupyter Notebook\")\n",
    "    # Little hack to avoid the \"OSError: Background processes not supported.\" error in Jupyter notebooks\"\n",
    "    import os\n",
    "    get_ipython().system = os.system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ⚠️ If you're using **Colab**, please, **restart the runtime** once everything above is installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ollama Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the ollama server first, as a background process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!OLLAMA_KEEP_ALIVE=-1 ollama serve > /dev/null 2>&1 &\n",
    "!sleep 10  # Let's wait a bit for the server to start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change the default sdialog model to `gemma3:12b`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sdialog\n",
    "\n",
    "sdialog.config.llm(\"ollama:gemma3:12b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Output (Dialogue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin by defining the JSON objects that we will use to represent the generated dialogues. For now this object will have only three fields: `\"model\"`, `\"seed\"`, `\"scenario\"`, and `\"dialog\"` to store the name of the model and the seed used to generate the dialogue, as well as the scenario associated to the dialogue and the dialogue itself, respectively. More preciselly, the `\"dialog\"` field will contain the list of turns of the conversation in order, with the speaker name and the corresponding utterances. As shown in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_dialogue = {\n",
    "    \"model\": \"qwen2.5:14b\",  # the model used to generate the dialogue\n",
    "    \"seed\": 123,  # the seed used to generated\n",
    "    \"scenario\": \"short hello and good bye conversation\",  # the scenario used to generated the dialogue\n",
    "    \"turns\": [\n",
    "        {\"speaker\": \"Alice\", \"text\": \"Hey Bob!\"},\n",
    "        {\"speaker\": \"Bob\", \"text\": \"Hey Alice!\"},\n",
    "        {\"speaker\": \"Alice\", \"text\": \"Bye Bob!\"},\n",
    "        {\"speaker\": \"Bob\", \"text\": \"Bye bye!\"},\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `pydantic` to properly define our `Dialogue` type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List, Union, Optional\n",
    "\n",
    "class Turn(BaseModel):\n",
    "    speaker: str\n",
    "    text: str\n",
    "\n",
    "class Dialog(BaseModel):\n",
    "    model: str  # the model used to generate the dialogue\n",
    "    seed: int  # the seed used to generated\n",
    "    scenario: Optional[Union[dict, str]] = None  # the scenario used to generated the dialogue\n",
    "    turns: List[Turn]  # the list of turns of the conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a Python `pydantic` class to formally represent our dialogues is quite useful, we can convert any JSON dialogue to our `Dialog` class as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dialog(model='qwen2.5:14b', seed=123, scenario='short hello and good bye conversation', turns=[Turn(speaker='Alice', text='Hey Bob!'), Turn(speaker='Bob', text='Hey Alice!'), Turn(speaker='Alice', text='Bye Bob!'), Turn(speaker='Bob', text='Bye bye!')])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dialogue = Dialog.model_validate(example_dialogue)\n",
    "my_dialogue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or the opposite, convert our `Dialog`s to a `dict` or a JSON as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'qwen2.5:14b',\n",
       " 'seed': 123,\n",
       " 'scenario': 'short hello and good bye conversation',\n",
       " 'turns': [{'speaker': 'Alice', 'text': 'Hey Bob!'},\n",
       "  {'speaker': 'Bob', 'text': 'Hey Alice!'},\n",
       "  {'speaker': 'Alice', 'text': 'Bye Bob!'},\n",
       "  {'speaker': 'Bob', 'text': 'Bye bye!'}]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dialogue.model_dump()  # a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"model\": \"qwen2.5:14b\",\n",
      "  \"seed\": 123,\n",
      "  \"scenario\": \"short hello and good bye conversation\",\n",
      "  \"turns\": [\n",
      "    {\n",
      "      \"speaker\": \"Alice\",\n",
      "      \"text\": \"Hey Bob!\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Bob\",\n",
      "      \"text\": \"Hey Alice!\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Alice\",\n",
      "      \"text\": \"Bye Bob!\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Bob\",\n",
      "      \"text\": \"Bye bye!\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "my_dialogue_json = my_dialogue.model_dump_json(indent=2)  # a string containing the dialog as a JSON object\n",
    "print(my_dialogue_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, of course, create a new `Dialog` from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dialog(model='qwen2.5:14b', seed=123, scenario=None, turns=[Turn(speaker='Alice', text='Hi :)'), Turn(speaker='Bob', text='Bye! :(')])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dialog(\n",
    "    model=\"qwen2.5:14b\",\n",
    "    seed=123,\n",
    "    turns=[\n",
    "        Turn(speaker=\"Alice\", text=\"Hi :)\"),\n",
    "        Turn(speaker=\"Bob\", text=\"Bye! :(\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternativelly, we can use the built-in `Dialog` class from `sdialog`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dialog(version='0.4.1+fdf16e10134b2ac7dea23518390cf5b09aa10380', timestamp='2025-11-21T10:12:08Z', model='qwen2.5:14b', seed=123, id='97c70f30-7a3e-4c6e-a7d8-4b0ca2e7d767', parentId=None, complete=None, personas=None, context=None, scenario='short hello and good bye conversation', turns=[Turn(speaker='Alice', text='Hey Bob!'), Turn(speaker='Bob', text='Hey Alice!'), Turn(speaker='Alice', text='Bye Bob!'), Turn(speaker='Bob', text='Bye bye!')], events=None, notes=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sdialog import Dialog\n",
    "\n",
    "my_dialog = Dialog.model_validate(example_dialogue)\n",
    "my_dialog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which besides providing the exact same functionalities, among other things, allow us to:\n",
    "\n",
    "- Pretty print the dialogue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m[dialog_id] \u001b[35m97c70f30-7a3e-4c6e-a7d8-4b0ca2e7d767\u001b[0m\n",
      "\u001b[1m\u001b[95m[model] \u001b[35mqwen2.5:14b\u001b[0m\n",
      "\u001b[1m\u001b[95m[seed] \u001b[35m123\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Begins ---\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mHey Bob!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mHey Alice!\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mBye Bob!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mBye bye!\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Ends ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "my_dialog.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Print it in a vanilla textual form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice: Hey Bob!\n",
      "Bob: Hey Alice!\n",
      "Alice: Bye Bob!\n",
      "Bob: Bye bye!\n"
     ]
    }
   ],
   "source": [
    "print(my_dialog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Save it to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# either as a JSON object\n",
    "my_dialog.to_file(\"output/my_dialogue.json\")\n",
    "\n",
    "# or a txt file\n",
    "my_dialog.to_file(\"output/my_dialogue.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_(check created files [`output/my_dialogue.json`](output/my_dialogue.json) and [`output/my_dialogue.txt`](output/my_dialogue.txt))_\n",
    "\n",
    "- Load a dialogue from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m[dialog_id] \u001b[35m97c70f30-7a3e-4c6e-a7d8-4b0ca2e7d767\u001b[0m\n",
      "\u001b[1m\u001b[95m[model] \u001b[35mqwen2.5:14b\u001b[0m\n",
      "\u001b[1m\u001b[95m[seed] \u001b[35m123\u001b[0m\n",
      "\u001b[1m\u001b[95m[scenario] \u001b[35m\u001b[0m\n",
      "\u001b[35mshort hello and good bye conversation\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Begins ---\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mHey Bob!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mHey Alice!\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mBye Bob!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mBye bye!\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Ends ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "my_dialog = Dialog.from_file(\"output/my_dialogue.json\")\n",
    "my_dialog.print(scenario=True)  # `scenario=True` to also print the metadata stored in scenario field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m[dialog_id] \u001b[35m262292f0-b751-4774-9598-877f33558a86\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Begins ---\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mHey Bob!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mHey Alice!\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mBye Bob!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mBye bye!\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Ends ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "my_dialog = Dialog.from_file(\"output/my_dialogue.txt\")\n",
    "my_dialog.print(scenario=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Or simple things like quickly know how long a dialogue is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_dialog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to begin working on synthetic `Dialog` (;)) generation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dialogue Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description-driven Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `sdialog`'s built-in `DialogueGenerator` class that we can instantiate using descriptions to generate our dialogues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdialog.generators import DialogGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which takes the following arguments as input:\n",
    "- `model` model name to use (any model tag from [ollama hub](https://ollama.com/library)).\n",
    "- `dialogue_details` the details about the desired dialogue.\n",
    "- `output_format` the output format as a `pydantic` class or JSON scheme, as we did above (`LLMDialogOutput` by default).\n",
    "- `scenario` an optional metadata field that describes the scenario used to generated dialogue.\n",
    "\n",
    "For instance, let's create an instance of `DialogGenerator` to generate conversations between Bob and Alice about her birthday:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-21 11:12:08] INFO:sdialog.util:Loading Ollama model: gemma3:12b\n"
     ]
    }
   ],
   "source": [
    "dialog_generator = DialogGenerator(\n",
    "    dialogue_details=\"The conversation is between a dad (Bob) and his doughter (Alice). \"\n",
    "                     \"Her birthday is coming up and she wants to throw a Star Wars themed party.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can use the build-in `.generate()` method to generate conversations for such instance:\n",
    "\n",
    "_(each time you run the code below, a different dialogue will be generated)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m[dialog_id] \u001b[35mcd02e639-d4b8-4f47-ab06-e125c0a43735\u001b[0m\n",
      "\u001b[1m\u001b[95m[model] \u001b[35m{'name': 'ollama:gemma3:12b', 'seed': 13, 'top_p': 0.95, 'temperature': 1, 'top_k': 64}\u001b[0m\n",
      "\u001b[1m\u001b[95m[seed] \u001b[35m296780219\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Begins ---\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mHey Dad! Guess what?\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mHey sweetie! What's got you so excited?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mMy birthday is getting closer! And I was thinking... I really, *really* want to have a Star Wars themed party!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mA Star Wars party? Wow, that's ambitious! Sounds like fun, though. You’re a big Star Wars fan, aren't you?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mThe biggest! I want everything to be Star Wars! Decorations, games, even the food!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mOkay, okay! Let's not get carried away just yet. A Star Wars party is going to take some planning. What did you have in mind?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mI was thinking we could have lightsaber training! And a scavenger hunt for droid parts! And maybe a Death Star piñata?\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mA Death Star piñata? That’s brilliant! Lightsaber training sounds like a bit of a challenge for us adults, but we'll figure something out. What about food? Star Wars themed snacks?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mYeah! Wookiee cookies! Yoda soda! And maybe blue milk?\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mBlue milk? I'm not sure I'm ready for blue milk. Let’s research that one. So, we're thinking decorations, games, food... invitations too, right?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mDefinitely! I want them to look like Imperial Decrees! Or maybe holographic invitations!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mHolographic invitations? That's getting pretty advanced, sweetie. Maybe we can design something cool online. We can do this! It's going to be a lot of work, but it's your birthday. Let's make a list of what we need to do.\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mYay! You’ll help me? That's the best birthday present ever!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mOf course, I will! Let's start looking at some ideas online tonight. Okay, I'm getting hungry, let’s go grab a snack. How about some… regular milk?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mHaha, okay Dad! But we *are* having blue milk for the party!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mWe're going to see about that! Alright, love you!\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mLove you too, Dad! Star Wars party, here we come!\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Ends ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dialog = dialog_generator.generate()\n",
    "dialog.print(scenario=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now change the description, now the party has to be about Lord of the Rings, not Star Wars. To do this we can simply pass the new description when calling the generate method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m[dialog_id] \u001b[35mbe322ef8-011d-4013-9029-7883b9d65bdb\u001b[0m\n",
      "\u001b[1m\u001b[95m[model] \u001b[35m{'name': 'ollama:gemma3:12b', 'seed': 13, 'top_p': 0.95, 'temperature': 1, 'top_k': 64}\u001b[0m\n",
      "\u001b[1m\u001b[95m[seed] \u001b[35m1364604708\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Begins ---\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mHey Dad! Guess what?\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mHey sweetie! What’s got you so excited?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mMy birthday is getting closer, and I was thinking… I really want to have a Lord of the Rings party!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mA Lord of the Rings party? Wow, that's ambitious! That's a lot more than just balloons and cake.\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mI know! But I’m so excited about it! We could decorate like the Shire, and maybe even have a costume contest!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mA costume contest? Okay, that sounds pretty fun. But, are you sure you have enough friends who would be into dressing up as hobbits or elves?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mPretty much everyone in my class loves Lord of the Rings! We watch the movies all the time. And I’m already brainstorming costume ideas!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mAlright, alright, you're convincing me. What about food? Lembas bread isn't exactly easy to bake.\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mHaha, no Lembas bread! But we could have 'second breakfast' with pastries and fruit. And maybe a 'feast' with themed snacks. I saw some really cool ideas online!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mOkay, online research is good. Let's look at some of these ideas together. How about we start with a budget? These themed parties can get expensive.\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mGood idea! I'm willing to help with some of the costs, too. Maybe I can do some of the decorations myself?\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mThat would be fantastic! Let's make a list of what we need and how much we’re willing to spend. We can look at it this weekend.\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mYay! This is going to be the best birthday ever! Thanks, Dad!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mYou're welcome, sweetie. Now, let’s get those ideas down and start planning! Just try not to get *too* swept away by Middle-earth. We still have chores to do, you know.\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mHaha, okay, okay! Talk to you later!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mSee ya, love! Have fun dreaming up those hobbit costumes!\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Ends ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dialog = dialog_generator.generate(\n",
    "    \"The conversation is between a dad (Bob) and his doughter (Alice). \"\n",
    "    \"Her birthday is coming up and she wants to throw a Lord of the Rings themed party.\"\n",
    ")\n",
    "dialog.print(scenario=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the `seed` number above to re-generate the exact same dialogue each time as an argument of `generate()`, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m[dialog_id] \u001b[35m62070161-0671-499e-9c0f-0448ac085160\u001b[0m\n",
      "\u001b[1m\u001b[95m[model] \u001b[35m{'name': 'ollama:gemma3:12b', 'seed': 13, 'top_p': 0.95, 'temperature': 1, 'top_k': 64}\u001b[0m\n",
      "\u001b[1m\u001b[95m[seed] \u001b[35m1364604708\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Begins ---\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mHey Dad! Guess what?\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mHey sweetie! What’s got you so excited?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mMy birthday is getting closer, and I was thinking… I really want to have a Lord of the Rings party!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mA Lord of the Rings party? Wow, that's ambitious! That's a lot more than just balloons and cake.\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mI know! But I’m so excited about it! We could decorate like the Shire, and maybe even have a costume contest!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mA costume contest? Okay, that sounds pretty fun. But, are you sure you have enough friends who would be into dressing up as hobbits or elves?\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mPretty much everyone in my class loves Lord of the Rings! We watch the movies all the time. And I’m already brainstorming costume ideas!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mAlright, alright, you're convincing me. What about food? Lembas bread isn't exactly easy to bake.\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mHaha, no Lembas bread! But we could have 'second breakfast' with pastries and fruit. And maybe a 'feast' with themed snacks. I saw some really cool ideas online!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mOkay, online research is good. Let's look at some of these ideas together. How about we start with a budget? These themed parties can get expensive.\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mGood idea! I'm willing to help with some of the costs, too. Maybe I can do some of the decorations myself?\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mThat would be fantastic! Let's make a list of what we need and how much we’re willing to spend. We can look at it this weekend.\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mYay! This is going to be the best birthday ever! Thanks, Dad!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mYou're welcome, sweetie. Now, let’s get those ideas down and start planning! Just try not to get *too* swept away by Middle-earth. We still have chores to do, you know.\u001b[0m\n",
      "\u001b[31m[Alice] \u001b[0mHaha, okay, okay! Talk to you later!\u001b[0m\n",
      "\u001b[94m[Bob] \u001b[0mSee ya, love! Have fun dreaming up those hobbit costumes!\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Ends ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dialog_generator.generate(\n",
    "        \"The conversation is between a dad (Bob) and his doughter (Alice). \"\n",
    "        \"Her birthday is coming up and she wants to throw a Lord of the Rings themed party.\",\n",
    "        seed=1364604708\n",
    ").print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Role-Playing-based Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal here will be to have to LLM to generate the dialogues by role-playing the different charecters.\n",
    "\n",
    "Each character will be fully defined by its persona, so, the same way started this tutorial by defining what a \"synthetic `Dialog`\" will actually be, we should now define our `Persona`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, the `sdialog` contains a `BasePersona` that we can import to create our own custom persona classes, let's import it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdialog.personas import BasePersona"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's define our concrete `Persona` class now by specifying some useful attributes like a name, role, background, etc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Persona(BasePersona):\n",
    "    name: str = \"\"\n",
    "    role: str = \"\"\n",
    "    background: str = \"\"\n",
    "    personality: str = \"\"\n",
    "    circumstances: str = \"\"\n",
    "    rules: str = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create/instantiate any `Persona` we want for our characters. Let's create our Bob and Alice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "bob_persona = Persona(\n",
    "        name=\"Bob\",\n",
    "        role=\"great dad\",\n",
    "        circumstances=\"Your daughter will talk to you\",\n",
    "        background=\"Computer Science PhD.\",\n",
    "        personality=\"an extremely happy person that likes to help people\",\n",
    ")\n",
    "\n",
    "alice_persona = Persona(\n",
    "    name=\"Alice\",\n",
    "    role=\"lovely daughter\",\n",
    "    circumstances=\"Your birthday is getting closer and you are talking with your dad to organize the party.\"\n",
    "                  \"You want your party to be themed as Lord of The Rings.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we print `bob` we will see that it is automatically converted to natural language description, which is usefull when we want to create the actual prompt for our LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Name: Bob\n",
      "* Role: great dad\n",
      "* Background: Computer Science PhD.\n",
      "* Personality: an extremely happy person that likes to help people\n",
      "* Circumstances: Your daughter will talk to you\n"
     ]
    }
   ],
   "source": [
    "print(bob_persona)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also pretty print it with `.print()` method, as with dialog objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[35m--- Persona Begins ---\u001b[0m\n",
      "\u001b[31m[Name] \u001b[0mBob\u001b[0m\n",
      "\u001b[31m[Role] \u001b[0mgreat dad\u001b[0m\n",
      "\u001b[31m[Background] \u001b[0mComputer Science PhD.\u001b[0m\n",
      "\u001b[31m[Personality] \u001b[0man extremely happy person that likes to help people\u001b[0m\n",
      "\u001b[31m[Circumstances] \u001b[0mYour daughter will talk to you\u001b[0m\n",
      "\u001b[31m[Rules] \u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Persona Ends ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "bob_persona.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case we are working with a really complex persona and this default description is not good for your needs, you can overwrite it by defining your own `description()` method as in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your awesome name is Bob and your awesome role is being a great dad\n"
     ]
    }
   ],
   "source": [
    "class PersonaCustom(BasePersona):\n",
    "    name: str = \"\"\n",
    "    role: str = \"\"\n",
    "\n",
    "    def description(self):\n",
    "        return f\"Your awesome name is {self.name} and your awesome role is being a {self.role}\"\n",
    "\n",
    "awesome_bob = PersonaCustom(\n",
    "        name=\"Bob\",\n",
    "        role=\"great dad\"\n",
    ")\n",
    "\n",
    "# Let's print \"awesome_bob\" persona\n",
    "print(awesome_bob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we now know how to create personas, let's move to the fun part which is actually creating the actual generator.\n",
    "\n",
    "Fortunatelly, we can simply use `sdialog`'s built-in `PersonaDialogGenerator` class to generate our persona-based dialogues as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-21 11:15:35] INFO:sdialog.util:Loading Ollama model: gemma3:12b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m[dialog_id] \u001b[35m2442f84c-00f1-4e14-a0ae-5c7b56cbbe80\u001b[0m\n",
      "\u001b[1m\u001b[95m[model] \u001b[35m{'name': 'ollama:gemma3:12b', 'seed': 13, 'temperature': 1, 'top_k': 64, 'top_p': 0.95}\u001b[0m\n",
      "\u001b[1m\u001b[95m[seed] \u001b[35m3259483948\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Begins ---\u001b[0m\n",
      "\u001b[94m[SPEAKER_B] \u001b[0mHi Dad!\u001b[0m\n",
      "\u001b[31m[SPEAKER_A] \u001b[0mAlice! Hi sweetie! How are you doing today?\u001b[0m\n",
      "\u001b[94m[SPEAKER_B] \u001b[0mI’m great! I was wondering if we could talk about my birthday party?\u001b[0m\n",
      "\u001b[31m[SPEAKER_A] \u001b[0mAbsolutely! I'm all ears. What's on your mind?\u001b[0m\n",
      "\u001b[94m[SPEAKER_B] \u001b[0mI really, really want to have a Lord of the Rings theme!\u001b[0m\n",
      "\u001b[31m[SPEAKER_A] \u001b[0mA Lord of the Rings party! That's fantastic! That's such a brilliant idea, I love it! We can really do something special. What were you thinking?\u001b[0m\n",
      "\u001b[94m[SPEAKER_B] \u001b[0mI thought we could decorate like the Shire! And maybe have a costume contest?\u001b[0m\n",
      "\u001b[31m[SPEAKER_A] \u001b[0mThe Shire decorations are perfect! And a costume contest? Oh, that's going to be a riot! I can already picture it - little hobbits running around!\u001b[0m\n",
      "\u001b[94m[SPEAKER_B] \u001b[0mYeah! And maybe a 'quest' for the kids to complete?\u001b[0m\n",
      "\u001b[31m[SPEAKER_A] \u001b[0mA quest! Brilliant! We can hide little 'rings' around the yard, and they have to find them all. I'm getting excited just thinking about it!\u001b[0m\n",
      "\u001b[94m[SPEAKER_B] \u001b[0mI know, right? It’s going to be so much fun! Do you think you could help me with the decorations?\u001b[0m\n",
      "\u001b[31m[SPEAKER_A] \u001b[0mOf course, sweetie! I'm happy to help however I can. Let's start brainstorming ideas for food too! Lembas bread, perhaps?\u001b[0m\n",
      "\u001b[94m[SPEAKER_B] \u001b[0mYes! Lembas bread is a must! And maybe some seed cakes?\u001b[0m\n",
      "\u001b[31m[SPEAKER_A] \u001b[0mPerfect! We're going to have an epic party. I’m so glad you came up with this idea! It's going to be a fantastic memory.\u001b[0m\n",
      "\u001b[94m[SPEAKER_B] \u001b[0mThank you, Dad! I love you!\u001b[0m\n",
      "\u001b[31m[SPEAKER_A] \u001b[0mI love you too, sweetie! Now, let's get to work planning this amazing Lord of the Rings adventure!\u001b[0m\n",
      "\u001b[94m[SPEAKER_B] \u001b[0mOkay! Bye, Dad!\u001b[0m\n",
      "\u001b[31m[SPEAKER_A] \u001b[0mBye, sweetie! Have a wonderful day!\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Ends ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sdialog.generators import PersonaDialogGenerator\n",
    "\n",
    "dialog_generator = PersonaDialogGenerator(\n",
    "    persona_a=bob_persona,\n",
    "    persona_b=alice_persona,\n",
    ")\n",
    "\n",
    "dialog_generator.generate().print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case: Dialogue Generation for STAR Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin this section, make sure you have the STAR dataset downloaded in your system, inside the `datasets` folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's clone the STAR dataset repository\n",
    "!git clone https://github.com/RasaHQ/STAR.git datasets/STAR\n",
    "\n",
    "# Let's check that `dialogues` and `tasks` folders are inside `datasets/STAR`\n",
    "!ls datasets/STAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [STAR](https://arxiv.org/pdf/2010.11853) dataset contains 6652 human-generated dialogues as JSON objects where files are named as `NUMBER.json`.\n",
    "\n",
    "Humans had to follow a well-defined set of instruction to generate the dialogue role playing the system (wizard) and the client (user).\n",
    "\n",
    "For instance, clicking [here](datasets/STAR/dialogues/1.json) we can open the file [`1.json`](datasets/STAR/dialogues/1.json) containing the first dialogue. For now, let's focus only on the `\"Scenario\"` field.\n",
    "\n",
    "For instance, for the dialogue in `1.json` it is as follows:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"Domains\": [  # List of domains\n",
    "        \"doctor\"\n",
    "    ],\n",
    "    \"Happy\": true,  # Wheather or not the dialogue follos a happy path\n",
    "    \"MultiTask\": false,  # Wheather or not this dialogue involves more than one task\n",
    "    \"UserTask\": \"You (Alexis) had an appointment with Dr. Morgan the other day. Unfortunately, you forgot to write down the instructions the doctor gave you. Please followup and find out how often to take your medicine.\",\n",
    "    \"WizardTask\": \"Inform the user of his/her doctor's orders.\",\n",
    "    \"WizardCapabilities\": [  # List of flowcharts describing the each task the Wizard is cable of doing\n",
    "        {\n",
    "        \"Domain\": \"doctor\",\n",
    "        \"SchemaImage\": \"doctor_followup.jpg\",\n",
    "        \"Task\": \"doctor_followup\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "We can use the `STAR` module from `sdialog` to read scenarios object from any dialogue in STAR given it's id given a STAR conversation id as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Domains': ['doctor'],\n",
       " 'Happy': True,\n",
       " 'MultiTask': False,\n",
       " 'UserTask': 'You (Alexis) had an appointment with Dr. Morgan the other day. Unfortunately, you forgot to write down the instructions the doctor gave you. Please followup and find out how often to take your medicine.',\n",
       " 'WizardCapabilities': [{'Domain': 'doctor',\n",
       "   'SchemaImage': 'doctor_followup.jpg',\n",
       "   'Task': 'doctor_followup'}],\n",
       " 'WizardTask': \"Inform the user of his/her doctor's orders.\"}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sdialog.datasets import STAR\n",
    "\n",
    "# Let's first indicate where the dataset is located\n",
    "STAR.set_path(\"datasets/STAR/\")\n",
    "\n",
    "# Let's set the first dialogue as the target example\n",
    "TARGET_DIALOG = 1\n",
    "\n",
    "# Let's load the scenario of the first dialog\n",
    "scenario = STAR.get_dialog_scenario(TARGET_DIALOG)\n",
    "scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which corresponds to the following dialogue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m[dialog_id] \u001b[35m1\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Begins ---\u001b[0m\n",
      "\u001b[94m[User] \u001b[0mHello, I'm really worried. I forgot what I'm supposed to do and forgot to write it down... What do I do?\u001b[0m\n",
      "\u001b[31m[System] \u001b[0mCould I get your name, please?\u001b[0m\n",
      "\u001b[94m[User] \u001b[0mMy name is Alexis and my last doctor was Dr. Morgan, but now my doctor is Dr. Johnson and I forgot how to take my medicine.\u001b[0m\n",
      "\u001b[31m[System] \u001b[0mYour instructions are: Take your medicine before you go to sleep. If you experience nausea, please contact your doctor immediately..\u001b[0m\n",
      "\u001b[94m[User] \u001b[0mAre you sure I'm supposed to take it before bed? I don't go to sleep every day because my sleep schedule is totally off right now because of the Coronavirus.\u001b[0m\n",
      "\u001b[31m[System] \u001b[0mYes. It must be before bed or it will not be effective.\u001b[0m\n",
      "\u001b[94m[User] \u001b[0mOkay thank you. I will get back in touch if this doesn't help.\u001b[0m\n",
      "\u001b[31m[System] \u001b[0mThank you and goodbye.\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Ends ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "original_dialogue = STAR.get_dialog(1)\n",
    "original_dialogue.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description-driven Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `scenario`, we can see that in this conversation, the user's behavior is defined by instructions given in natural language (`\"UserTask\"`), however, the system/wizard behavior is more rigidly defined as a graph describing the dialogue policy to followed (since system was expected to be more deterministic). These graphs are described as JSON objects storing the graph edges as key:value pairs (source:destination). We can find these graphs in the [`STAR/tasks`](datasets/STAR/tasks) folder.\n",
    "\n",
    "Ideally, we would like our `DialogGenerator` to generate dialogues for each different scenario. That is, given a `scenario` we would like generate multiple dialogues for it.\n",
    "\n",
    "To achieve this, we only need to find a way to describe each `scenario` using natural language so that we can pass it to our `DialogGenerator`.\n",
    "\n",
    "Fortunately, we can use the built-in `get_scenario_description()` method to do this, which takes an `scenario` as input and returns its natural language description containing all the details (including the system behavior described by the graphs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The conversation is between a User and a AI assistant in the following domains: doctor.\n",
      "\n",
      "The User instructions are: You (Alexis) had an appointment with Dr. Morgan the other day. Unfortunately, you forgot to write down the instructions the doctor gave you. Please followup and find out how often to take your medicine.\n",
      "The AI assistant instructions are: Inform the user of his/her doctor's orders.\n",
      "\n",
      "In addition, the AI assistant is instructed to follow specific flowcharts to address the tasks.\n",
      "Flowcharts are defined as graph described using DOT.\n",
      "The actual DOT for the current tasks are:\n",
      "\n",
      "The graph for the task 'doctor_followup' with domain 'doctor' is:\n",
      "```dot\n",
      "digraph doctor_followup  {\n",
      "    hello -> ask_name;\n",
      "    ask_name -> doctor_ask_doctor_name;\n",
      "    doctor_ask_doctor_name -> query;\n",
      "    query -> doctor_inform_doctors_instructions;\n",
      "    doctor_inform_doctors_instructions -> anything_else\n",
      "}\n",
      "```\n",
      "and one example responses for each node is provided in the following json:\n",
      "```json\n",
      "{\n",
      "  \"hello\": \"Hello, how can I help?\",\n",
      "  \"ask_name\": \"Could I get your name, please?\",\n",
      "  \"doctor_ask_doctor_name\": \"Who is your doctor?\",\n",
      "  \"doctor_inform_doctors_instructions\": \"Your instructions are: INSTRUCTIONS.\",\n",
      "  \"doctor_bye\": \"Thank you and goodbye.\",\n",
      "  \"anything_else\": \"Is there anything else that I can do for you?\"\n",
      "}\n",
      "```\n",
      "\n",
      "# flake8: noqa: E501\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Finally, the following should be considered regarding the conversation:\n",
      "   1. The conversation follows the 'happy path', ] meaning the conversations goes according to what it is described in the flowcharts.\n",
      "   2. The user is calling to perform only the defined task (doctor_followup), nothing else.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(STAR.get_scenario_description(scenario))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the original graph in JSON describing the system's behavior have been converted to a [DOT](https://en.wikipedia.org/wiki/DOT_(graph_description_language)) description which should be easier to interpret by the LLM, since DOT is a well-known format to describe graphs in plain text. \n",
    "\n",
    "Let's now put these two methods together and create a function that given a STAR dialogue ID will generate a natural language description of the scenario associated to it, simply as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dialog_scenario_description(dialogue_id):\n",
    "    # Get the scenario of the target dialogue\n",
    "    scenario = STAR.get_dialog_scenario(dialogue_id)\n",
    "    # Then return it along its description in natural language\n",
    "    return scenario, STAR.get_scenario_description(scenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this function now we have everything we need to generate synthethic dialogues for STAR that follows the same scenario as a given target real dialogue.\n",
    "\n",
    "For instance, let's say we want to generate dialogues following the same scenario as the first STAR dialogue, we can simply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-21 11:16:11] INFO:sdialog.util:Loading Ollama model: gemma3:12b\n"
     ]
    }
   ],
   "source": [
    "# First, let's get the scenario and description of the first dialogue\n",
    "scenario, description = get_dialog_scenario_description(dialogue_id=1)\n",
    "\n",
    "# le'ts now create a dialogue generator for it\n",
    "dialog_generator = DialogGenerator(\n",
    "    dialogue_details=description,\n",
    "    scenario=scenario\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now generate multiple conversation that follows the same scenario as dialogue 1 of STAR dataset.\n",
    "> **Note**\n",
    "> Run the cell multiple times to get different conversations for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m[dialog_id] \u001b[35mab87f4ce-95ac-4470-b62d-27a90bb1d22a\u001b[0m\n",
      "\u001b[1m\u001b[95m[model] \u001b[35m{'name': 'ollama:gemma3:12b', 'seed': 13, 'top_p': 0.95, 'temperature': 1, 'top_k': 64}\u001b[0m\n",
      "\u001b[1m\u001b[95m[seed] \u001b[35m2131212703\u001b[0m\n",
      "\u001b[1m\u001b[95m[scenario] \u001b[35m\u001b[0m\n",
      "\u001b[35m{\n",
      "  \"Domains\": [\n",
      "    \"doctor\"\n",
      "  ],\n",
      "  \"Happy\": true,\n",
      "  \"MultiTask\": false,\n",
      "  \"UserTask\": \"You (Alexis) had an appointment with Dr. Morgan the other day. Unfortunately, you forgot to write down the instructions the doctor gave you. Please followup and find out how often to take your medicine.\",\n",
      "  \"WizardCapabilities\": [\n",
      "    {\n",
      "      \"Domain\": \"doctor\",\n",
      "      \"SchemaImage\": \"doctor_followup.jpg\",\n",
      "      \"Task\": \"doctor_followup\"\n",
      "    }\n",
      "  ],\n",
      "  \"WizardTask\": \"Inform the user of his/her doctor's orders.\"\n",
      "}\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Begins ---\u001b[0m\n",
      "\u001b[31m[AI assistant] \u001b[0mHello, how can I help?\u001b[0m\n",
      "\u001b[94m[Alexis] \u001b[0mHi, I had an appointment with Dr. Morgan recently and I unfortunately forgot to write down the instructions she gave me. I need to know how often to take my medicine.\u001b[0m\n",
      "\u001b[31m[AI assistant] \u001b[0mCould I get your name, please?\u001b[0m\n",
      "\u001b[94m[Alexis] \u001b[0mIt's Alexis.\u001b[0m\n",
      "\u001b[31m[AI assistant] \u001b[0mWho is your doctor?\u001b[0m\n",
      "\u001b[94m[Alexis] \u001b[0mDr. Morgan.\u001b[0m\n",
      "\u001b[31m[AI assistant] \u001b[0mYour instructions are: Take one tablet every 12 hours with food.\u001b[0m\n",
      "\u001b[94m[Alexis] \u001b[0mOh, fantastic! Thank you so much.\u001b[0m\n",
      "\u001b[31m[AI assistant] \u001b[0mIs there anything else that I can do for you?\u001b[0m\n",
      "\u001b[94m[Alexis] \u001b[0mNo, that's all. Thank you again.\u001b[0m\n",
      "\u001b[31m[AI assistant] \u001b[0mThank you and goodbye.\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Ends ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dialog = dialog_generator.generate()\n",
    "dialog.print(scenario=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the LLM is able to follow the scenario surprisingly well, specially for the system part which is guided by a graph with pre-defined responses.\n",
    "\n",
    "Now update the generator to match a more challenging scenario, let's say that of dialogue 5100 that is multi-task and does not follow a happy path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-21 11:16:20] INFO:sdialog.util:Loading Ollama model: gemma3:12b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Domains': ['plane', 'weather'],\n",
       " 'Happy': False,\n",
       " 'MultiTask': True,\n",
       " 'UserTask': 'Come up with your own scenario!\\n\\nAbout you:\\n- Your name: Ben\\n\\n The AI Assistant can handle:\\n- Search for a flight (e.g. from Chicago to Pittsburgh)\\n- Book a flight (e.g. with id 193)\\n- Checking the weather forecast in different Cities (e.g. Chicago or Pittsburgh)',\n",
       " 'WizardCapabilities': [{'Domain': 'plane',\n",
       "   'SchemaImage': 'plane_search.jpg',\n",
       "   'Task': 'plane_search'},\n",
       "  {'Domain': 'plane', 'SchemaImage': 'plane_book.jpg', 'Task': 'plane_book'},\n",
       "  {'Domain': 'weather', 'SchemaImage': 'weather.jpg', 'Task': 'weather'}],\n",
       " 'WizardTask': 'Follow the flow charts and help the user.'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scenario, description = get_dialog_scenario_description(5100)\n",
    "\n",
    "dialog_generator = DialogGenerator(\n",
    "    dialogue_details=description,\n",
    "    scenario=scenario\n",
    ")\n",
    "scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And generate dialogues for it:\n",
    "\n",
    "_(run multi-times the call to generate different ones)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m[dialog_id] \u001b[35m8d7211d0-4cfe-44ed-bdc2-dfd12ed7b6d6\u001b[0m\n",
      "\u001b[1m\u001b[95m[model] \u001b[35m{'name': 'ollama:gemma3:12b', 'seed': 13, 'temperature': 1, 'top_k': 64, 'top_p': 0.95}\u001b[0m\n",
      "\u001b[1m\u001b[95m[seed] \u001b[35m637994336\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Begins ---\u001b[0m\n",
      "\u001b[31m[Ben] \u001b[0mHello, how can I help?\u001b[0m\n",
      "\u001b[94m[User] \u001b[0mHi Ben, I'm Sarah. I'm calling about a flight and then the weather.\u001b[0m\n",
      "\u001b[31m[Ben] \u001b[0mMay I have your name, please?\u001b[0m\n",
      "\u001b[94m[User] \u001b[0mIt's Sarah, as I just said.\u001b[0m\n",
      "\u001b[31m[Ben] \u001b[0mWhere are you departing from?\u001b[0m\n",
      "\u001b[94m[User] \u001b[0mChicago.\u001b[0m\n",
      "\u001b[31m[Ben] \u001b[0mWhere are you going to?\u001b[0m\n",
      "\u001b[94m[User] \u001b[0mPittsburgh.\u001b[0m\n",
      "\u001b[31m[Ben] \u001b[0mOn what day do you want the plane to arrive?\u001b[0m\n",
      "\u001b[94m[User] \u001b[0mNext Friday, please.\u001b[0m\n",
      "\u001b[31m[Ben] \u001b[0mI can also filter the results for a specific airline, class, price or duration.\u001b[0m\n",
      "\u001b[94m[User] \u001b[0mNo, just give me the cheapest option. And now, can you check the weather in Pittsburgh for the same day?\u001b[0m\n",
      "\u001b[31m[Ben] \u001b[0mRight, I found CLAZZ flight with AIRLINE for PRICE credits that takes DURATION hours to get to ARRIVAL_CITY.\u001b[0m\n",
      "\u001b[94m[User] \u001b[0mActually, forget the flight. It’s too expensive. Can you tell me the weather?\u001b[0m\n",
      "\u001b[31m[Ben] \u001b[0mFor what day would you like the weather forecast?\u001b[0m\n",
      "\u001b[94m[User] \u001b[0mNext Friday.\u001b[0m\n",
      "\u001b[31m[Ben] \u001b[0mFor what location would you like the weather forecast?\u001b[0m\n",
      "\u001b[94m[User] \u001b[0mPittsburgh.\u001b[0m\n",
      "\u001b[31m[Ben] \u001b[0mIt will be WEATHER all day on DAY in CITY, with temperatures of around TEMPERATURE degrees celsius.\u001b[0m\n",
      "\u001b[94m[User] \u001b[0mOkay, that’s helpful. I think I'll book that flight from earlier instead. Can you help me with that?\u001b[0m\n",
      "\u001b[31m[Ben] \u001b[0mCan I have your flight ID, please?\u001b[0m\n",
      "\u001b[94m[User] \u001b[0mI don’t have a flight ID. I didn’t book one!\u001b[0m\n",
      "\u001b[31m[Ben] \u001b[0mSorry, but the flight with id 'FLIGHT_ID' is not available any more.\u001b[0m\n",
      "\u001b[94m[User] \u001b[0mOh, great. Never mind. Thank you and goodbye.\u001b[0m\n",
      "\u001b[31m[Ben] \u001b[0mThank you and goodbye.\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Ends ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dialog_generator.generate().print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Role-playing-based Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before, in previous section we only had to find a way to describe each `scenario` using natural language so that we can pass it to our `DialogGenerator`.\n",
    "\n",
    "Likewise, now we have to find a way to create the right system and user personas for each scenario which means we have to return the right system and user `Persona`s for a given `scenario`.\n",
    "\n",
    "Fortunately, we can use the built-in `STAR.get_user_persona_for_scenario(scenario)` and `STAR.get_system_persona_for_scenario(scenario)` methods to achieve this.\n",
    "\n",
    "For instance, let's get the user persona for the `scenario` of the first dialogue above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Language: English\n",
      "* Role: user calling a AI assistant that can perform multiple tasks in the following domains: doctor.\n",
      "\n",
      "The following should be considered regarding the conversation:\n",
      "   1. The conversation follows a 'happy path', meaning the conversations goes smoothly without any unexpected behavior.\n",
      "   2. The conversation involves only one task you were instructed to (doctor_followup), nothing else\n",
      "* Circumstances: You (Alexis) had an appointment with Dr. Morgan the other day. Unfortunately, you forgot to write down the instructions the doctor gave you. Please followup and find out how often to take your medicine.\n"
     ]
    }
   ],
   "source": [
    "scenario = STAR.get_dialog_scenario(TARGET_DIALOG)\n",
    "\n",
    "user_persona = STAR.get_user_persona_for_scenario(scenario)\n",
    "print(user_persona)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, similar to what we did in the previous subsection, we just need to define a function that given a dialogue ID can return its scenario as well as the system and user persona for it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dialog_scenario_and_personas(dialogue_id):\n",
    "    # Get the scenario of the target dialogue\n",
    "    scenario = STAR.get_dialog_scenario(dialogue_id)\n",
    "    # Get the personas\n",
    "    system = STAR.get_system_persona_for_scenario(scenario)\n",
    "    user = STAR.get_user_persona_for_scenario(scenario)\n",
    "    return scenario, system, user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it, not we can simply create a `PersonaDialogGenerator` using the system and user personas as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-21 11:16:39] INFO:sdialog.util:Loading Ollama model: gemma3:12b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m[dialog_id] \u001b[35m84838b85-328a-420e-ac9c-0c62f5deeef8\u001b[0m\n",
      "\u001b[1m\u001b[95m[model] \u001b[35m{'name': 'ollama:gemma3:12b', 'seed': 13, 'top_k': 64, 'top_p': 0.95, 'temperature': 1}\u001b[0m\n",
      "\u001b[1m\u001b[95m[seed] \u001b[35m983680493\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Begins ---\u001b[0m\n",
      "\u001b[31m[SPEAKER_A] \u001b[0mHello, how can I help?\u001b[0m\n",
      "\u001b[94m[SPEAKER_B] \u001b[0mHi! I need to know what instructions Dr. Morgan gave me during my appointment. I forgot to write them down.\u001b[0m\n",
      "\u001b[31m[SPEAKER_A] \u001b[0mCould I get your name, please?\u001b[0m\n",
      "\u001b[94m[SPEAKER_B] \u001b[0mIt's Alexis.\u001b[0m\n",
      "\u001b[31m[SPEAKER_A] \u001b[0mWho is your doctor?\u001b[0m\n",
      "\u001b[94m[SPEAKER_B] \u001b[0mDr. Morgan.\u001b[0m\n",
      "\u001b[31m[SPEAKER_A] \u001b[0mYour instructions are: Take one tablet twice a day, with food. Specifically, one tablet at 8 AM and one tablet at 6 PM. It is important to adhere to this schedule for best results.\u001b[0m\n",
      "\u001b[94m[SPEAKER_B] \u001b[0mOh, that's perfect! Thank you so much!\u001b[0m\n",
      "\u001b[31m[SPEAKER_A] \u001b[0mIs there anything else that I can do for you?\u001b[0m\n",
      "\u001b[94m[SPEAKER_B] \u001b[0mNo, that's all. Thanks again!\u001b[0m\n",
      "\u001b[31m[SPEAKER_A] \u001b[0mThank you and goodbye.\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Dialogue Ends ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# let's get the personas and the scenario\n",
    "scenario, system, user = get_dialog_scenario_and_personas(dialogue_id=1)\n",
    "\n",
    "# le'ts now create a dialogue generator for it\n",
    "dialog_generator = PersonaDialogGenerator(\n",
    "    persona_a=system,\n",
    "    persona_b=user,\n",
    "    scenario=scenario\n",
    ")\n",
    "\n",
    "# let's generate the dialogue\n",
    "dialog_generator.generate().print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it for this tutorial, congrats! 😎"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving our dialogues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's generate one synthetic dialog for each happy `\"doctor_followup\"` dialog in STAR and save it to disk for later use.\n",
    "\n",
    "Let's first get all happy dialogues for this task using `sdialog`'s built-in `STAR.get_dialogs()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of happy \"doctor_followup\" dialogues in STAR: 105\n"
     ]
    }
   ],
   "source": [
    "original_dialogs = STAR.get_dialogs(task_name=\"doctor_followup\", happy=True, multitask=False)\n",
    "print('Total number of happy \"doctor_followup\" dialogues in STAR:', len(original_dialogs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate the dialogues and save them in the path pointed by the `PATH_OUTPUT` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "PATH_OUTPUT = \"output/STAR/full-generation\"\n",
    "\n",
    "path_txt = os.path.join(PATH_OUTPUT, \"txt\")\n",
    "path_json = os.path.join(PATH_OUTPUT, \"json\")\n",
    "os.makedirs(path_txt, exist_ok=True)\n",
    "os.makedirs(path_json, exist_ok=True)\n",
    "\n",
    "for dialog in tqdm(original_dialogs, desc=\"Dialog generation\"):\n",
    "    if os.path.exists(os.path.join(path_txt, f\"{dialog.id}.txt\")):\n",
    "        continue\n",
    "\n",
    "    scenario, description = STAR.get_dialog_scenario_description(dialog.id)\n",
    "    dialog_generator = DialogGenerator(\n",
    "        dialogue_details=description,\n",
    "        scenario=scenario\n",
    "    )\n",
    "    dialog = dialog_generator.generate(id=dialog.id, seed=dialog.id)\n",
    "\n",
    "    # Normalize speaker names in each turn (since their also generated by the LLM)\n",
    "    for turn in dialog.turns:\n",
    "        turn.speaker = \"System\" if \"AI\" in turn.speaker else \"User\"\n",
    "\n",
    "    dialog.to_file(os.path.join(path_json, f\"{dialog.id}.json\"))\n",
    "    dialog.to_file(os.path.join(path_txt, f\"{dialog.id}.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's check the files were generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls output/STAR/full-generation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls output/STAR/full-generation/txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, huh? Congrats for finalizing the tutorial! you did a great job! 😎"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
