{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3: Audio Generation with Room Accoustic\n",
    "\n",
    "<p align=\"right\" style=\"margin-right: 8px;\">\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/idiap/sdialog/blob/main/tutorials/01_audio/3.accoustic_simulation-customer_service.ipynb\">\n",
    "        <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "    </a>\n",
    "</p>\n",
    "\n",
    "## Getting started\n",
    "\n",
    "### Environment Setup\n",
    "\n",
    "Let's first check if our environment is all set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the environment depending on weather we are running in Google Colab or Jupyter Notebook\n",
    "import os\n",
    "from IPython import get_ipython\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "if \"google.colab\" in str(get_ipython()):\n",
    "    print(\"Running on CoLab\")\n",
    "\n",
    "    !sudo apt-get install sox ffmpeg\n",
    "    !sudo apt-get -qq -y install espeak-ng > /dev/null 2>&1\n",
    "    %pip install -q kokoro>=0.9.4\n",
    "    # Installing sdialog\n",
    "    !git clone https://github.com/idiap/sdialog.git\n",
    "    %cd sdialog\n",
    "    %pip install -e .[audio]\n",
    "    %cd ..\n",
    "else:\n",
    "    print(\"Running in Jupyter Notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the example dialogue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run the next steps in a fast manner, we will start from an existing dialog generated using previous tutorials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdialog import Dialog\n",
    "\n",
    "path_dialog = \"../../tests/data/customer_support_dialogue.json\"\n",
    "\n",
    "if not os.path.exists(path_dialog):\n",
    "    !wget https://raw.githubusercontent.com/idiap/sdialog/refs/heads/main/tests/data/customer_support_dialogue.json\n",
    "    path_dialog = \"./customer_support_dialogue.json\"\n",
    "\n",
    "original_dialog = Dialog.from_file(path_dialog)\n",
    "original_dialog.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instanciate voices database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdialog.audio.voice_database import HuggingfaceVoiceDatabase\n",
    "kokoro_voice_database = HuggingfaceVoiceDatabase(\"sdialog/voices-kokoro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instanciate TTS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdialog.audio.tts import KokoroTTS\n",
    "tts_engine = KokoroTTS()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup stage: Audio Dialog and Audio Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdialog.audio.dialog import AudioDialog\n",
    "from sdialog.audio.pipeline import AudioPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the original dialog into a audio enhanced dialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialog: AudioDialog = AudioDialog.from_dialog(original_dialog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciate the audio pipeline in order to use `Kokoro` (`tts_engine`) as the TTS model and save the audios outputs of all the dialogs into the directory `./audio_outputs`.\n",
    "\n",
    "The voices are sampled from the `kokoro_voice_database` based on the persona attributes `age`, `gender` and `language`, as assigned during the original textual dialog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"./audio_outputs_customer_support\", exist_ok=True)\n",
    "audio_pipeline = AudioPipeline(\n",
    "    voice_database=kokoro_voice_database,\n",
    "    tts_engine=tts_engine,\n",
    "    dscaper_data_path=\"./dscaper_data_customer_support\",\n",
    "    dir_audio=\"./audio_outputs_customer_support\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate the sound events database (for example the background sound of the customer support room with phone ringing, people talking, ...)\n",
    "# audio_pipeline.populate_dscaper([\"<username>/<the_name_of_the_dataset>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or if you encounter any issue during the download due to timeout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "!hf download sdialog/background --repo-type dataset\n",
    "!hf download sdialog/foreground --repo-type dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate a medical room it will be enough and display it's shape and content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdialog.audio.utils import SourceVolume, SourceType\n",
    "from sdialog.audio.jsalt import MedicalRoomGenerator, RoomRole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "room = MedicalRoomGenerator().generate(args={\"room_type\": RoomRole.EXAMINATION})\n",
    "img = room.to_image()\n",
    "display(img)\n",
    "img.save(\"room.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also optionaly position the speakers 1 and 2 into the room by assigning them places around available furnitures in the room or to a 3D position in the environment.\n",
    "\n",
    "By default the speakers are positionned around the center of the room.\n",
    "\n",
    "Here the `MedicalRoomGenerator` is generating rooms with a predefined list of furnitures (desk, sink, door, ...) that can be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdialog.audio.room import SpeakerSide, Role, RoomPosition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "room.place_speaker_around_furniture(speaker_name=Role.SPEAKER_1, furniture_name=\"desk\", max_distance=1.0, side=SpeakerSide.FRONT)\n",
    "room.place_speaker_around_furniture(speaker_name=Role.SPEAKER_2, furniture_name=\"desk\", max_distance=1.0, side=SpeakerSide.BACK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the new positions of the speakers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = room.to_image()\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the inference of the audio pipeline on the previously converted dialog. In this case we will focus on generating the \"unprocessed\" audio, which consist of the agregation of all utterances from the dialog. Rather than using the dialog identifier as the name of the directory, we are using here a custom directory name `demo_dialog_room_accoustic` which will be saved at `./audio_outputs_customer_support/demo_dialog_room_accoustic/`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialog: AudioDialog = audio_pipeline.inference(\n",
    "    dialog,\n",
    "    perform_room_acoustics=True,\n",
    "    environment={\n",
    "        \"room\": room, # Need to provide a room object to trigger the 3rd step of the audio pipeline\n",
    "        \"background_effect\": \"white_noise\",\n",
    "        \"foreground_effect\": \"ac_noise_minimal\",\n",
    "        \"foreround_effect_position\": RoomPosition.TOP_RIGHT,\n",
    "        \"source_volumes\": {\n",
    "            SourceType.ROOM: SourceVolume.HIGH,\n",
    "            SourceType.BACKGROUND: SourceVolume.VERY_LOW\n",
    "        },\n",
    "        \"kwargs_pyroom\": {\n",
    "            \"ray_tracing\": True,\n",
    "            \"air_absorption\": True\n",
    "        }\n",
    "    },\n",
    "    dialog_dir_name=\"demo_dialog_room_accoustic\",\n",
    "    room_name=\"my_room_config_1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialog.display()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
