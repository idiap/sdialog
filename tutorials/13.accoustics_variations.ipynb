{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SDialog dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the environment depending on weather we are running in Google Colab or Jupyter Notebook\n",
    "import os\n",
    "from IPython import get_ipython\n",
    "\n",
    "if \"google.colab\" in str(get_ipython()):\n",
    "    print(\"Running on CoLab\")\n",
    "\n",
    "    # Installing sdialog\n",
    "    !git clone https://github.com/qanastek/sdialog.git\n",
    "    %cd sdialog\n",
    "    %pip install -e .\n",
    "    %cd ..\n",
    "else:\n",
    "    print(\"Running in Jupyter Notebook\")\n",
    "    # Little hack to avoid the \"OSError: Background processes not supported.\" error in Jupyter notebooks\"\n",
    "    get_ipython().system = os.system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `.venv` using the root `requirement.txt` file and Python `3.11.14`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdialog import Dialog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load an existing dialogue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run the next steps in a fast manner, we will start from an existing dialog generated using previous tutorials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dialog = \"../tests/data/demo_dialog_doctor_patient.json\"\n",
    "\n",
    "if not os.path.exists(path_dialog) and not os.path.exists(\"./demo_dialog_doctor_patient.json\"):\n",
    "    !wget https://raw.githubusercontent.com/qanastek/sdialog/refs/heads/main/tests/data/demo_dialog_doctor_patient.json\n",
    "    path_dialog = \"./demo_dialog_doctor_patient.json\"\n",
    "\n",
    "original_dialog = Dialog.from_file(path_dialog)\n",
    "original_dialog.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 15: Variation of rooms setups and their impact on accoustic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind this tutorial is to demonstrate how different room configurations and their acoustic properties can influence the quality and characteristics of generated dialogue audio.\n",
    "\n",
    "By comparing the audio results generated with different room configurations, you will be able to hear and understand how the acoustic environment affects the perception and quality of synthetic dialogues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instanciate voices database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdialog.audio.voice_database import HuggingfaceVoiceDatabase\n",
    "kokoro_voice_database = HuggingfaceVoiceDatabase(\"sdialog/voices-kokoro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instanciate TTS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q kokoro>=0.9.4\n",
    "!apt-get -qq -y install espeak-ng > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdialog.audio.tts_engine import KokoroTTS\n",
    "tts_engine = KokoroTTS()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup stage: Audio Dialog and Audio Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdialog.audio.dialog import AudioDialog\n",
    "from sdialog.audio.pipeline import AudioPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the original dialog into a audio enhanced dialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialog: AudioDialog = AudioDialog.from_dialog(original_dialog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciate the audio pipeline in order to use `Kokoro` (`tts_engine`) as the TTS model and save the audios outputs of all the dialogs into the directory `./audio_outputs`.\n",
    "\n",
    "The voices are sampled from the `kokoro_voice_database` based on the persona attributes `age`, `gender` and `language`, as assigned during the original textual dialog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/cyrta/dscaper.git\n",
    "!pip install -e ./dscaper/\n",
    "!pip install scaper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get `ModuleNotFoundError: No module named 'scaper'` just `Restart` your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scaper\n",
    "DATA_PATH = \"./dscaper_data\" # Path where the sound events, utterances and timelines database will be saved\n",
    "os.makedirs(DATA_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsc = scaper.Dscaper(dscaper_base_path=DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"./audio_outputs_variations\", exist_ok=True)\n",
    "audio_pipeline = AudioPipeline(\n",
    "    voice_database=kokoro_voice_database,\n",
    "    tts_pipeline=tts_engine,\n",
    "    dscaper=dsc,\n",
    "    dir_audio=\"./audio_outputs_variations\",\n",
    ")\n",
    "# audio_pipeline = AudioPipeline() # Can also be used with default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate the sound events database\n",
    "audio_pipeline.populate_dscaper([\"sdialog/background\",\"sdialog/foreground\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or if you encounter any issue during the download due to timeout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "!hf download sdialog/background --repo-type dataset\n",
    "!hf download sdialog/foreground --repo-type dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate a medical room it will be enough and display it's shape and content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "from sdialog.audio.room import DirectivityType\n",
    "from sdialog.audio.utils import SourceVolume, SourceType\n",
    "from sdialog.audio.room_generator import BasicRoomGenerator\n",
    "from sdialog.audio.jsalt import MedicalRoomGenerator, RoomRole\n",
    "from sdialog.audio.room import SpeakerSide, Role, RoomPosition, MicrophonePosition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run steps 1 and 2 before, since they are commonly shared with all our simulations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialog: AudioDialog = audio_pipeline.inference(\n",
    "    dialog,\n",
    "    do_step_1=True,\n",
    "    do_step_2=True,\n",
    "    do_step_3=False,\n",
    "    dialog_dir_name=\"demo_dialog_room_accoustic\",\n",
    "    audio_file_format=\"mp3\"\n",
    ")\n",
    "\n",
    "print(\"dialog.audio_step_1_filepath\",dialog.audio_step_1_filepath)\n",
    "print(\"dialog.audio_step_2_filepath\",dialog.audio_step_2_filepath)\n",
    "\n",
    "display(Audio(dialog.audio_step_1_filepath, autoplay=False, rate=24000))\n",
    "display(Audio(dialog.audio_step_2_filepath, autoplay=False, rate=24000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, run the accoustics simulation for all the room role with have here. Since we used the same `dialog_dir_name` as before (`demo_dialog_room_accoustic`) for steps 1 and 2, we will have access to the data obtains by those two process and only run the 3rd step:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also do it with basic rooms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _size in range(20, 40, 5):\n",
    "\n",
    "    room = BasicRoomGenerator().generate(args={\"room_size\": _size})\n",
    "\n",
    "    room.place_speaker_around_furniture(speaker_name=Role.SPEAKER_1, furniture_name=\"center\", max_distance=5.0, side=SpeakerSide.FRONT)\n",
    "    room.place_speaker_around_furniture(speaker_name=Role.SPEAKER_2, furniture_name=\"center\", max_distance=5.0, side=SpeakerSide.BACK)\n",
    "\n",
    "    room.set_directivity(direction=DirectivityType.OMNIDIRECTIONAL)\n",
    "\n",
    "    room.set_mic_position(MicrophonePosition.CEILING_CENTERED)\n",
    "\n",
    "    dialog: AudioDialog = audio_pipeline.inference(\n",
    "        dialog,\n",
    "        environment={\n",
    "            \"room\": room, # Need to provide a room object to trigger the 3rd step of the audio pipeline\n",
    "            \"background_effect\": \"white_noise\",\n",
    "            \"foreground_effect\": \"ac_noise_minimal\",\n",
    "            \"foreround_effect_position\": RoomPosition.TOP_LEFT,\n",
    "            \"source_volumes\": {\n",
    "                SourceType.ROOM: SourceVolume.HIGH,\n",
    "                SourceType.BACKGROUND: SourceVolume.VERY_LOW\n",
    "            },\n",
    "            \"kwargs_pyroom\": {\n",
    "                \"ray_tracing\": True,\n",
    "                \"air_absorption\": True\n",
    "            }\n",
    "        },\n",
    "        do_step_1=False,\n",
    "        do_step_2=False,\n",
    "        do_step_3=True,\n",
    "        dialog_dir_name=\"demo_dialog_room_accoustic\",\n",
    "        room_name=f\"my_room_config_BasicRoom_{_size}\",\n",
    "        audio_file_format=\"mp3\"\n",
    "    )\n",
    "\n",
    "    print(f\"Done with {_size} basic room configuration!\")\n",
    "    print(\"\\n\"*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\"*25)\n",
    "print(\"- Room Configurations\")\n",
    "print(\"-\"*25)\n",
    "for config_name in dialog.audio_step_3_filepaths:\n",
    "    print(f\"> Room Configuration: {config_name}\")\n",
    "    display(Audio(dialog.audio_step_3_filepaths[config_name][\"audio_path\"], autoplay=False, rate=24000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _role in [_r for _r in RoomRole]:\n",
    "\n",
    "    room = MedicalRoomGenerator().generate(args={\"room_type\": _role})\n",
    "\n",
    "    room.place_speaker_around_furniture(speaker_name=Role.SPEAKER_1, furniture_name=\"desk\", max_distance=1.0, side=SpeakerSide.FRONT)\n",
    "    room.place_speaker_around_furniture(speaker_name=Role.SPEAKER_2, furniture_name=\"desk\", max_distance=1.5, side=SpeakerSide.BACK)\n",
    "\n",
    "    room.set_directivity(direction=DirectivityType.OMNIDIRECTIONAL)\n",
    "\n",
    "    room.set_mic_position(MicrophonePosition.CHEST_POCKET_SPEAKER_1)\n",
    "\n",
    "    dialog: AudioDialog = audio_pipeline.inference(\n",
    "        dialog,\n",
    "        environment={\n",
    "            \"room\": room, # Need to provide a room object to trigger the 3rd step of the audio pipeline\n",
    "            \"background_effect\": \"white_noise\",\n",
    "            \"foreground_effect\": \"ac_noise_minimal\",\n",
    "            \"foreround_effect_position\": RoomPosition.TOP_LEFT,\n",
    "            \"source_volumes\": {\n",
    "                SourceType.ROOM: SourceVolume.HIGH,\n",
    "                SourceType.BACKGROUND: SourceVolume.VERY_LOW\n",
    "            },\n",
    "            \"kwargs_pyroom\": {\n",
    "                \"ray_tracing\": True,\n",
    "                \"air_absorption\": True\n",
    "            }\n",
    "        },\n",
    "        do_step_1=False,\n",
    "        do_step_2=False,\n",
    "        do_step_3=True,\n",
    "        dialog_dir_name=\"demo_dialog_room_accoustic\",\n",
    "        room_name=f\"my_room_config_{_role}\",\n",
    "        audio_file_format=\"mp3\"\n",
    "    )\n",
    "\n",
    "    print(f\"Done with {_role} room configuration!\")\n",
    "    print(\"\\n\"*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\"*25)\n",
    "print(\"- Room Configurations\")\n",
    "print(\"-\"*25)\n",
    "for config_name in dialog.audio_step_3_filepaths:\n",
    "    print(f\"> Room Configuration: {config_name}\")\n",
    "    display(Audio(dialog.audio_step_3_filepaths[config_name][\"audio_path\"], autoplay=False, rate=24000))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
