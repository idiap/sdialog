{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SDialog dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the environment depending on weather we are running in Google Colab or Jupyter Notebook\n",
    "from IPython import get_ipython\n",
    "\n",
    "\n",
    "if \"google.colab\" in str(get_ipython()):\n",
    "    print(\"Running on CoLab\")\n",
    "\n",
    "    # Installing Ollama (if you are not planning to use Ollama, you can just comment these lines to speed up the installation)\n",
    "    !curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "    # Installing sdialog\n",
    "    !git clone https://github.com/qanastek/sdialog.git\n",
    "    %cd sdialog\n",
    "    %pip install -e .\n",
    "    %cd ..\n",
    "else:\n",
    "    print(\"Running in Jupyter Notebook\")\n",
    "    # Little hack to avoid the \"OSError: Background processes not supported.\" error in Jupyter notebooks\"\n",
    "    import os\n",
    "    get_ipython().system = os.system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run following commands and then `Restart` your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "%pip install -e ..\n",
    "%pip show sdialog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 7: Advanced audio generation and room accoustic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run the next steps in a fast manner, we will start from an existing dialog generated using previous tutorials. If you haven't download yet `dialog_demo.json` locally, please download it from our GitHub repository using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If dialog_demo.json is not present, download it\n",
    "if os.path.exists(\"dialog_demo.json\"):\n",
    "    print(\"dialog_demo.json already exists\")\n",
    "else:\n",
    "    !wget https://raw.githubusercontent.com/qanastek/sdialog/refs/heads/main/misc/audio/dialog_demo.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sdialog\n",
    "from sdialog import Dialog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dialog = Dialog.from_file(\"dialog_demo.json\")\n",
    "original_dialog.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instanciate voices database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdialog.audio.voice_database import HuggingfaceVoiceDatabase\n",
    "dummy_voice_database = HuggingfaceVoiceDatabase(\"sdialog/voices-kokoro\")\n",
    "dummy_voice_database.get_voice(genre=\"male\", age=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instanciate TTS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q kokoro>=0.9.4 soundfile\n",
    "!apt-get -qq -y install espeak-ng > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdialog.audio.tts_engine import KokoroTTS\n",
    "tts_engine = KokoroTTS()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup stage: Audio Dialog and Audio Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdialog.audio.audio_dialog import AudioDialog\n",
    "from sdialog.audio.audio_pipeline import AudioPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the original dialog into a audio enhanced dialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dialog: AudioDialog = AudioDialog.from_dialog(original_dialog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciate the audio pipeline in order to use `Kokoro` (`tts_engine`) as the TTS model and save the audios outputs of all the dialogs into the directory `./audio_outputs`.\n",
    "\n",
    "The voices are sampled from the `dummy_voice_database` based on the persona attributes `age` and `gender`, as assigned during the original textual dialog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/cyrta/dscaper.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -e ./dscaper/\n",
    "!pip install scaper\n",
    "!pip install sox\n",
    "!pip install jams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scaper\n",
    "DATA_PATH = \"./dscaper_data\" # Path where the sound events, utterances and timelines database will be saved\n",
    "os.makedirs(DATA_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsc = scaper.Dscaper(dscaper_base_path=DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"./audio_outputs\", exist_ok=True)\n",
    "audio_pipeline = AudioPipeline(\n",
    "    voice_database=dummy_voice_database,\n",
    "    tts_pipeline=tts_engine,\n",
    "    dscaper=dsc,\n",
    "    dir_audio=\"./audio_outputs\",\n",
    ")\n",
    "# audio_pipeline = AudioPipeline() # Can also be used with default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate the sound events database\n",
    "audio_pipeline.populate_dscaper([\"sdialog/background\",\"sdialog/foreground\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from IPython.display import Audio, display\n",
    "from sdialog.audio.room import MicrophonePosition, Room, Dimensions3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the inference of the audio pipeline on the previously converted dialog. In this case we will focus on generating the \"unprocessed\" audio, which consist of the agregation of all utterances from the dialog. Rather than using the dialog identifier as the name of the directory, we are using here a custom directory name `demo_dialog_advanced_room_accoustic` which will be saved at `./audio_outputs/demo_dialog_advanced_room_accoustic/`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "microphone_positions = [MicrophonePosition.CEILING_CENTERED, MicrophonePosition.MONITOR]\n",
    "reverbs = [0.3, 0.5]\n",
    "DIR_NAME = f\"demo_dialog_advanced_room_accoustic_{time.time()}\"\n",
    "print(DIR_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate stage 1 and 2\n",
    "dialog: AudioDialog = audio_pipeline.inference(\n",
    "    audio_dialog,\n",
    "    do_step_1=True,\n",
    "    do_step_2=True,\n",
    "    do_step_3=False,\n",
    "    dialog_dir_name=DIR_NAME,\n",
    ")\n",
    "print(f\"Generating audio of stage 1 and 2 done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dialog.get_audio_sources())\n",
    "print(\"#\"*25)\n",
    "display(Audio(dialog.audio_step_1_filepath, autoplay=False, rate=24000))\n",
    "display(Audio(dialog.audio_step_2_filepath, autoplay=False, rate=24000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_ROOMS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate audio of stage 3 for each setup\n",
    "for _ in tqdm(range(NUMBER_OF_ROOMS)):\n",
    "\n",
    "    microphone_position = random.choice(microphone_positions)\n",
    "    reverberation_time_ratio = random.choice(reverbs)\n",
    "    \n",
    "    width = random.choice(range(2, 10))\n",
    "    length = random.choice(range(2, 10))\n",
    "    height = random.choice(range(2, 4))\n",
    "    room_dimensions = Dimensions3D(width=width, length=length, height=height)\n",
    "\n",
    "    timing = time.time()\n",
    "    room_name = f\"advanced_room_accoustic_{timing}\"\n",
    "\n",
    "    room = Room(\n",
    "        name=f\"room_{timing}\", # use time in seconds as the room name\n",
    "        description=\"description_of_the_room\",\n",
    "        dimensions=room_dimensions,\n",
    "        reverberation_time_ratio=reverberation_time_ratio,\n",
    "    )\n",
    "\n",
    "    print(\"-\"*25)\n",
    "    print(\"Before inference:\")\n",
    "    print(dialog.get_audio_sources())\n",
    "    print(\"-\"*25)\n",
    "\n",
    "    dialog: AudioDialog = audio_pipeline.inference(\n",
    "        dialog,\n",
    "        environment={\n",
    "            \"room\": room,\n",
    "            \"microphone_position\": microphone_position,\n",
    "        },\n",
    "        do_step_1=False,\n",
    "        do_step_2=False,\n",
    "        do_step_3=True,\n",
    "        dialog_dir_name=DIR_NAME,\n",
    "        room_name=room_name\n",
    "    )\n",
    "    print(f\"Room configuration: {room_name} - {microphone_position} - {width}x{length}={width*length}m² and {width*length*height}m³ - {reverberation_time_ratio}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\"*25)\n",
    "print(\"- Room Configurations\")\n",
    "print(\"-\"*25)\n",
    "for config_name in dialog.audio_step_3_filepaths:\n",
    "    print(f\"> Room Configuration: {config_name}\")\n",
    "    display(Audio(dialog.audio_step_3_filepaths[config_name][\"audio_path\"], autoplay=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jsalt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
